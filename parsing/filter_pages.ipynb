{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports \n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "import ast\n",
    "\n",
    "import string\n",
    "folder_prefix = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script filter_pages.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_pickle(folder_prefix + 'nowdata/charters_2015.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['WEBTEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['values', 'academics', 'academic', 'skills', 'skill', 'purpose', 'purposes',\n",
    "                       'direction', 'mission', 'vision', 'visions', 'missions',\n",
    "                       'ideals', 'cause', 'causes', 'curriculum', 'curricular',\n",
    "                       'method', 'methods', 'pedagogy', 'pedagogical', 'pedagogies', 'approach', 'approaches', 'model', 'models', 'system', 'systems',\n",
    "                       'structure', 'structures', 'philosophy', 'philosophical', 'philosophies', 'beliefs', 'believe', 'belief',\n",
    "                       'principles', 'principle', 'creed', 'creeds', 'credo', 'moral', 'morals', 'morality', 'history', 'histories', 'our story',\n",
    "                       'the story', 'school story', 'background', 'backgrounds', 'founding', 'founded', 'foundation', 'foundations', 'foundational',\n",
    "                       'established','establishment', 'our school began', 'we began',\n",
    "                       'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "                       'identity', 'identities', 'profile', 'highlights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_values = {'values':2, 'academics':1, 'academic':1, 'skills':1, 'skill':1, 'purpose':2, 'purposes':2,\n",
    "                       'direction':1, 'mission':2, 'vision':2, 'visions':2, 'missions':2,\n",
    "                       'ideals':2, 'cause':1, 'causes':1, 'curriculum':2, 'curricular':2,\n",
    "                       'method':1, 'methods':1, 'pedagogy':2, 'pedagogical':1, 'pedagogies':1, 'approach':1, 'approaches':1, 'model':2, 'models':2, 'system':2, 'systems':2,\n",
    "                       'structure':1, 'structures':1, 'philosophy':2, 'philosophical':2, 'philosophies':2, 'beliefs':2, 'believe':2, 'belief':2,\n",
    "                       'principles':2, 'principle':2, 'creed':2, 'creeds':2, 'credo':2, 'moral':2, 'morals':2, 'morality':2, 'history':1, 'histories':1, 'our story':1,\n",
    "                       'the story':1, 'school story':1, 'background':1, 'backgrounds':1, 'founding':1, 'founded':1, 'foundation':1, 'foundations':1, 'foundational':1,\n",
    "                       'established':1,'establishment':1, 'our school began':1, 'we began':1,\n",
    "                       'doors opened':1, 'school opened':1, 'about us':2, 'our school':1, 'who we are':1,\n",
    "                       'identity':1, 'identities':1, 'profile':1, 'highlights':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid approach\n",
    "\n",
    "# Separate keywords to be treated differently\n",
    "small_keywords = []\n",
    "large_keywords = []\n",
    "\n",
    "for entry in keywords:\n",
    "    small_keywords.append(entry) if len(entry.split()) < 3 else large_keywords.append(entry)\n",
    "\n",
    "large_words = [entry.split() for entry in large_keywords] # list words for each large dict entry\n",
    "large_lengths = [len(x) for x in large_words]\n",
    "large_first_words = [x[0] for x in large_words] # first words of each large entry in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_count2(text):\n",
    "\n",
    "    \"\"\"Hybrid of dict_count and dict_count1. \n",
    "    \n",
    "    Uses dict_count1 approach to count matches for entries with > 2 words in keywords.\n",
    "    Uses dict_count approach for all other entries.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = 0 # hitscore\n",
    "    splitted_phrase = re.split('\\W+|_', text.lower()) # Remove punctuation with regex that keeps only letters and spaces\n",
    "\n",
    "    for length in range(1, 3):\n",
    "        if len(splitted_phrase) < length:\n",
    "            continue # If text chunk is shorter than length of dict entries being matched, there are no matches.\n",
    "        for i in range(len(splitted_phrase) - length + 1):\n",
    "            entry = ' '.join(splitted_phrase[i:i+length]) # Builds chunk of 'length' words without ending space\n",
    "            if entry in keywords:\n",
    "                counts += keywords_values[entry]\n",
    "    mask = [[word == entry for word in splitted_phrase] for entry in large_first_words]\n",
    "    indices = np.transpose(np.nonzero(mask))\n",
    "    for ind in indices:\n",
    "        if ind[1] <= (len(splitted_phrase) - large_lengths[ind[0]]) and large_words[ind[0]] == splitted_phrase[ind[1] : ind[1] + large_lengths[ind[0]]]:\n",
    "            counts += keywords_values[large_keywords[ind[0]]]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pages(li_tuples, MIN_HITCOUNT = 1, MAX_NUMPAGES = 250):\n",
    "    \"\"\"\n",
    "    Takes in a list of quadruples\n",
    "    string texts from a school. Most likely from the WEBTEXT column\n",
    "    For the row, the function returns the top 250 pages of the list who have the highest hitcount    \n",
    "    \"\"\"\n",
    "    \n",
    "    #just keep track of the hit count\n",
    "    \n",
    "    #turn the list of quadruples into a list of just string texts  \n",
    "    \n",
    "    if len(li_tuples) == 0: #if taking in an emoty list, we return an empty list\n",
    "        return li_tuples\n",
    "    \n",
    "    school_pages = []\n",
    "    \n",
    "    for tup in li_tuples:\n",
    "        if len(tup) == 4:\n",
    "            school_pages.append(tup[3])\n",
    "    \n",
    "    \n",
    "    tuples = []\n",
    "    index = 0\n",
    "    for page in school_pages:\n",
    "        hit_count = dict_count2(page)\n",
    "        if hit_count >= MIN_HITCOUNT:\n",
    "            tuples.append([hit_count, index])\n",
    "    \n",
    "    #sort the tuples/sublists by highest to lowest hit count\n",
    "    #take the top 250 , or less is len(school_pages) < 250\n",
    "    sorted_tuples = sorted(tuples, key=getKey, reverse = True)\n",
    "    filtered_tuples = []\n",
    "    if len(sorted_tuples) < 250:\n",
    "        filtered_tuples = sorted_tuples\n",
    "    else:\n",
    "        filtered_tuples = sorted_tuples[:250]\n",
    "    \n",
    "    #get the page at the correpsonding index (tup[1]) from school_pages\n",
    "    final_pages = [school_pages[tup[1]] for tup in filtered_tuples] \n",
    "    \n",
    "    return final_pages\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pages2(school_pages, MIN_HITCOUNT = 1, MAX_NUMPAGES = 250, AGGRO = False, is_set = False):\n",
    "    \"\"\"Filters page text with hit count at least min hit count if school has more than MAX_NUMPAGES distinct pages else unfiltered of pages is returned.\n",
    "    \n",
    "    Returns max_numpages pages with priority given to higher hitscore and then lower page depth(even when AGGRO is TRUE). Boolean value returned is to help generate WEBTEXT_METHOD later.\n",
    "    school_pages: entry of 'webtext' column\n",
    "    is_set: True if school_pages is set of pages\n",
    "    aggro: When true, only pages that have >= MIN_HITCOUNT hits pass. Only resort to CMO pages when no pages pass\n",
    "    \"\"\"\n",
    "    if not is_set:\n",
    "        school_pages = set([p for p in school_pages])\n",
    "#     if len(pages) <= MAX_NUMPAGES:\n",
    "#         return ([(p.url, p.boo, p.depth, p.text) for p in pages], 0)\n",
    "    all_tuples = []\n",
    "    filtered_num = 0 # number of pages that passed the hitscore requirement\n",
    "    filtered = []\n",
    "    max_hc = -1\n",
    "    min_depth = 99999\n",
    "    for p in school_pages:\n",
    "        hit_count = dict_count2(p)\n",
    "        if hit_count >= MIN_HITCOUNT:\n",
    "            filtered.append(hit_count, p)\n",
    "            filtered_num += 1\n",
    "#         if max_hc < hit_count:\n",
    "#             max_hc = hit_count\n",
    "#         if min_depth > int(p.depth):\n",
    "#             min_depth = int(p.depth)\n",
    "        # maintain list containing all pages and corresponding hit scores\n",
    "        all_tuples.append((hit_count, (p.url, p.boo, p.depth, p.text)))\n",
    "    if not aggro and filtered_num and filtered_num <= MAX_NUMPAGES:\n",
    "            return ([t[1] for t in filtered], False)        \n",
    "    all_tuples = [(t[0] - .00001*int(t[1][2]), t[1]) for t in all_tuples] # prepare list to be heapified\n",
    "    if aggro:\n",
    "        all_tuples = filtered\n",
    "    # priority number is hit_count - .00001*page.depth so pages with high hitscores are prioritized followed by low page depths\n",
    "    filtered = [t[1] for t in q.nlargest(MAX_NUMPAGES, all_tuples)]\n",
    "    return (filtered, filtered_num == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['WEBTEXT'] = original_df['WEBTEXT'].apply(filter_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitered_df = original_df[['NCESSCH', 'WEBTEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = pd.read_csv(folder_prefix + 'nowdata/parsing/filtered_df.csv', sep=\"\\t\", low_memory=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filt_df['WEBTEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df['WEBTEXT'] = filt_df['WEBTEXT'].fillna('0')\n",
    "filt_df['WEBTEXT'] = filt_df['WEBTEXT'].apply(ast.literal_eval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filt_df['WEBTEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df['WEBTEXT'][0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see if Nan's on filt_df = original_df Nan's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df['WEBTEXT'][9] == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['NCESSCH'][0] == filt_df['NCESSCH'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df['WEBTEXT'][1] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['WEBTEXT'][1] == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_df['WEBTEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pgs_list = []\n",
    "for li in original_df['WEBTEXT']:\n",
    "    num_pgs_list.append(len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(num_pgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for num in num_pgs_list:\n",
    "    if num > 250:\n",
    "        i+=1\n",
    "print(i) # only 238 are greater than 250\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None == None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x=float('nan')\n",
    "math.isnan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df['WEBTEXT'][1] == x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(filt_df['NCESSCH'] == original_df['NCESSCH']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just check if the places where I have put 0 (was initially NaN) in WEBTEXT column in filt_df \n",
    "i = 0\n",
    "num_incorrect = 0\n",
    "for li in filt_df:\n",
    "    if li == 0 and not (original_df['WEBTEXT'][i] == \"\"):\n",
    "        print(num_incorrect)\n",
    "        num_incorrect+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = 0\n",
    "for li in filt_df['WEBTEXT']:\n",
    "    if li == 0:\n",
    "        nan_count+=1\n",
    "\n",
    "print(nan_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_count = 0\n",
    "for li in filt_df['WEBTEXT']:\n",
    "    if li == []:\n",
    "        li_count+=1\n",
    "\n",
    "print(li_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count + li_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(type(filt_df['WEBTEXT'][9]) == list) and filt_df['WEBTEXT'][9]!= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_list_count = 0\n",
    "for li in filt_df['WEBTEXT']:\n",
    "    if (type(li) == list) and li!= []:\n",
    "        actual_list_count+=1\n",
    "\n",
    "print(actual_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "for li in original_df['WEBTEXT']:\n",
    "    if li == \"\":\n",
    "        b+=1\n",
    "\n",
    "print(b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so Nan's are in the right place, stuff is filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = pd.read_csv(folder_prefix + 'nowdata/parsing/filtered_df.csv', sep=\"\\t\", low_memory=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
