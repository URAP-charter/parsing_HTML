{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import sched\n",
    "\n",
    "\n",
    "import logging\n",
    " \n",
    "# add filemode=\"w\" to overwrite\n",
    "#logging.basicConfig(filename=\"df_chunk_index.log\", level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(filename=\"rows_far.log\", level=logging.INFO)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "from difflib import SequenceMatcher as SeqMatcher\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for multiprocessing\n",
    "import os # For navigation\n",
    "\n",
    "\n",
    "folder_prefix = '/vol_b/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[48]:\n",
    "\n",
    "\n",
    "new_data = pd.read_csv(folder_prefix + \"current_df_WEBTEXT.csv\", sep=\"\\t\", low_memory=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "# keywords = ['values', 'academics', 'skills', 'purpose',\n",
    "#                        'direction', 'mission', 'vision', 'vision', 'mission', 'our purpose',\n",
    "#                        'our ideals', 'ideals', 'our cause', 'curriculum','curricular',\n",
    "#                        'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system',\n",
    "#                        'structure','philosophy', 'philosophical', 'beliefs', 'believe',\n",
    "#                        'principles', 'creed', 'credo', 'values','moral', 'history', 'our story',\n",
    "#                        'the story', 'school story', 'background', 'founding', 'founded',\n",
    "#                        'established','establishment', 'our school began', 'we began',\n",
    "#                        'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "#                        'our identity', 'profile', 'highlights']\n",
    "\n",
    "# mission_keywords = ['mission','vision', 'vision:', 'mission:', 'our purpose', 'our ideals', 'ideals:', 'our cause', 'cause:', 'goals', 'objective']\n",
    "# curriculum_keywords = ['curriculum', 'curricular', 'program', 'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system', 'structure']\n",
    "# philosophy_keywords = ['philosophy', 'philosophical', 'beliefs', 'believe', 'principles', 'creed', 'credo', 'value',  'moral']\n",
    "# history_keywords = ['history', 'story','our story', 'the story', 'school story', 'background', 'founding', 'founded', 'established', 'establishment', 'our school began', 'we began', 'doors opened', 'school opened']\n",
    "# about_keywords =  ['about us', 'our school', 'who we are', 'overview', 'general information', 'our identity', 'profile', 'highlights']\n",
    "\n",
    "# mission_keywords = set(stemmer.stem(word) for word in mission_keywords)\n",
    "# curriculum_keywords = set(stemmer.stem(word) for word in curriculum_keywords)\n",
    "# philosophy_keywords = set(stemmer.stem(word) for word in philosophy_keywords)\n",
    "# history_keywords = set(stemmer.stem(word) for word in history_keywords)\n",
    "# about_keywords =  set(stemmer.stem(word) for word in about_keywords)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# m = '[(\\'https://www.aaechighschools.com/\\', \\'False\\', \\'0\\', \"Home\\\\n | \\\\nParent / Student Login \\\\nCall Today!\\\\n602.297.8500\\\\nPrograms\\\\nAcademics\\\\nEquine Studies\\\\nMath & Sciences\\\\nVeterinary & Medical\\\\nOur Schools\\\\nSouth Mountain\\\\nCalendar\\\\nParadise Valley\\\\nCalendar\\\\nRed Mountain\\\\nCalendar\\\\nEstrella Mountain\\\\nCalendar\\\\nPrescott Valley\\\\nCalendar\\\\nMesa\\\\nCalendar\\\\nFor Parents\\\\nStudent Life\\\\nAbout AAEC\\\\nHistory & Philosophy\\\\nGoverning Board\\\\nCommunity Relations\\\\nRegistration\\\\nOther Scholarship Opportunities\\\\nResources\\\\nAccreditation\\\\nSPED\\\\nCareers\\\\nAAEC â\\x80\\x93 In the News\\\\nResource Guides\\\\nPolicies\\\\nFAQ\\\\nBlog\\\\nCommunity Service Events\\\\nAnnual Financial Reports\\\\nPre-Enroll\\\\nContact Us\\\\nBlog\\\\nMesa Campus Now Open!\\\\nAAEC is also building a brand new facility next to MCC in Mesa!!! Call today for details on how you can benefit from AAEC 480-222-3999.\\\\nJoin Today\\\\nSpring semester college classes have started for our High School students. Call today for details on how you can benefit from AAEC.\\\\nPrograms\\\\nStudents who enroll in our college prep programs can start earning college credits while in high school. \\\\nParents\\\\nAAEC is a leading college prep high school with locations in South Mountain, Red Mountain, Estrella Mountain, Paradise Valley and Prescott Valley.\\\\nSchools\\\\nArizona Agribusiness & Equine Center (AAEC) is a leading public charter school offering students the opportunity to earn college credits\\\\nAbout AAEC\\\\nAAEC high schools offer college preparatory curriculum, and enable students to earn college credits while completing work for their high school diploma.\\\\nRequest More Info\\\\n About Our\\\\n Programs!\\\\n\\\\t\\\\tName (*)\\\\nInvalid Input\\\\t\\\\t\\\\n\\\\t\\\\tEmail (*)\\\\nInvalid Input\\\\t\\\\t\\\\n\\\\t\\\\tPhone\\\\nInvalid Input\\\\t\\\\t\\\\n\\\\t\\\\tCampus\\\\nSouth Mountain\\\\nParadise Valley\\\\nRed Mountain\\\\nEstrella Mountain\\\\nPrescott Valley\\\\nMesa\\\\nInvalid Input\\\\t\\\\t\\\\nSouth Mountain\\\\nParadise Valley\\\\nRed Mountain\\\\nEstrella Mountain\\\\nPrescott Valley\\\\nMesa\\\\nAcademics\\\\nEquine Studies\\\\nMath & Sciences\\\\nVeterinary & Medical\\\\n\\\\r Public Early College High School\\\\nMission\\\\nAAEC Early College High School prepares young adults for success now and in the future by promoting lifelong learning through rigorous academic instruction, promoting social responsibility and employability, and providing motivated students with the opportunity to earn college credits while completing their high school requirements.\\\\nAAEC Early College High School vigorously participates in school-to-career initiatives and occupational education at state and national levels. Please visit the \\\\nArizona Department of Education\\\\n website to view AAEC\\'s most recent school report card.\\\\nEnroll in our \\\\ncollege preparatory classes\\\\n and start earning college credits today. Contact AAEC Early College High School for information about our \\\\ncampuses\\\\n, read our \\\\nFAQ\\\\n, or \\\\nenroll online\\\\n today.\\\\nHighly-Qualified Educators Who Make a Difference â\\x80¢ Rich Curriculum that Exceeds State Standards â\\x80¢ FREE COLLEGE TUITION*\\\\r\\\\n*For Qualified Arizona Resident Students\\\\nPrograms\\\\nOur Schools\\\\nFor Parents\\\\nStudent Life\\\\nAbout AAEC\\\\nSouth Mountain\\\\nParadise Valley\\\\nRed Mountain\\\\nEstrella Mountain\\\\nPrescott Valley\\\\nResources\\\\nTax Credit Form\\\\nPre-Enroll\\\\nSitemap\\\\nCareers\\\\nArizona Agribusiness & Equine Center\\\\n District Office\\\\n 3636 N Central Ave Suite 1050\\\\n Phoenix, AZ 85012\\\\n ph. 602-297-8500\\\\n fx. 602-297-8540\\\\nGoverning Board Meeting Agenda\\\\nÂ© Copyright 2013 by Arizona Agribusiness & Equine Center. All Rights Reserved\")'\n",
    "# rm = \n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# new_data.columns\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# k = new_data.iloc[7:11]['CMO_WEBTEXT'][0]\n",
    "# d = '(\\'https://'\n",
    "# s =  [d+e for e in k.split(d) if e]\n",
    "# #k.split(\"(\\'https://\")\n",
    "# k\n",
    "# pr = ast.literal_eval(k)\n",
    "# len(pr)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# #all_pages is a list of strings of all web page texts\n",
    "# all_pages = []\n",
    "\n",
    "# no_nan_data = new_data.loc[:, ['CMO_WEBTEXT']].fillna(\"\")\n",
    "# col_pages = no_nan_data.loc[1:3]['CMO_WEBTEXT'] #change to new_data['data'] later, but work with first 1 schools for now\n",
    "# i = 0\n",
    "# count = 0\n",
    "# for school_data in col_pages:\n",
    "\n",
    "#     for tup in school_data:\n",
    "#         all_pages.append(tup[3])\n",
    "      \n",
    "\n",
    "#     i+=1\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# unique_pages_set = set(all_pages)\n",
    "\n",
    "# #set already get rid of duplicate strings\n",
    "# ratio_df = pd.DataFrame(np.array(list(unique_pages_set)), columns=[\"Page\"])\n",
    "# ratio_df[\"Ratios\"] = np.nan\n",
    "# ratio_df[\"Similar\"] = np.nan #similar will hold similar indexes, > 0.90 ratios\n",
    "\n",
    "# ratio_df['Ratios'] = ratio_df['Ratios'].astype('object')\n",
    "# ratio_df['Similar'] = ratio_df['Similar'].astype('object')\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def create_sim_df(pages):\n",
    "    \n",
    "    ratio_df = pd.DataFrame(np.array(pages), columns=[\"Page\"])\n",
    "    ratio_df[\"Ratios\"] = np.nan\n",
    "    ratio_df[\"Similar\"] = np.nan #similar will hold similar indexes, > 0.90 ratios\n",
    "\n",
    "    ratio_df['Ratios'] = ratio_df['Ratios'].astype('object')\n",
    "    ratio_df['Similar'] = ratio_df['Similar'].astype('object')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (len(pages) <= 20) :\n",
    "   \n",
    "        for a in range(len(pages)):\n",
    "            sim_ind_list = []\n",
    "            for b in range(len(pages)):\n",
    "                #add on every other index execpt the one it's on\n",
    "                #this way, it'll compare every page to every other page for similarities\n",
    "                if (a != b):\n",
    "                    sim_ind_list.append(b)\n",
    "            ratio_df['Similar'][a] = np.asarray(sim_ind_list)    \n",
    "   \n",
    "    else :\n",
    "        index = 0\n",
    "        for page0 in pages:\n",
    "            ratios_list = []\n",
    "            for page1 in pages:\n",
    "                ratios_list.append(SeqMatcher(None, page0, page1).ratio())\n",
    "            ratio_df['Ratios'][index] = ratios_list\n",
    "            sim_ind_list = np.asarray(np.where((np.asarray(ratios_list) >= 0.7) & (np.asarray(ratios_list) != 1.0))[0]).tolist()\n",
    "            ratio_df['Similar'][index] = sim_ind_list\n",
    "            index+=1\n",
    "                \n",
    "    num_rows = ratio_df.shape[0] \n",
    "    tot_tuples = []\n",
    "\n",
    "    final_cut_strings = [None] * num_rows\n",
    "    #count_not_similar = 0  \n",
    "    #not_sim_list = []\n",
    "    sim_list = []\n",
    "\n",
    "    for r in range(num_rows):\n",
    "        if (len(ratio_df['Similar'][r]) != 0):\n",
    "            sim_list.append(r)\n",
    "            #print(\"has similar > 0.9 at index : \" + str(r))\n",
    "            list_cut = []\n",
    "            for ind in ratio_df['Similar'][r]:\n",
    "                list_of_triples = SeqMatcher(None, ratio_df['Page'][r], ratio_df['Page'][ind]).get_matching_blocks()\n",
    "                zeroth_triple = list_of_triples[0] #first triple, most likely in beginning, most likely a header\n",
    "                n = zeroth_triple[2] #j to j+n are the indices of the overlapping part\n",
    "                orig_string = ratio_df['Page'][ind]\n",
    "                cut_down_string = orig_string[:zeroth_triple[1]] + orig_string[zeroth_triple[1] + n:] #removes overlapping part\n",
    "                list_cut.append([ind, cut_down_string])\n",
    "            tot_tuples.extend(list_cut) #list of tuples\n",
    "        else:\n",
    "            #print(\"no similar at index : \" + str(r))\n",
    "            #not_sim_list.append(r)\n",
    "            #count_not_similar +=1\n",
    "            final_cut_strings[r]= ratio_df['Page'][r]\n",
    "    \n",
    "    return tot_tuples, final_cut_strings, ratio_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# index = 0\n",
    "# for page0 in unique_pages_set:\n",
    "#     ratios_list = []\n",
    "#     for page1 in unique_pages_set:\n",
    "#         ratios_list.append(SeqMatcher(None, page0, page1).ratio())\n",
    "#     ratio_df['Ratios'][index] = ratios_list\n",
    "#     sim_ind_list = np.asarray(np.where((np.asarray(ratios_list) > 0.9) & (np.asarray(ratios_list) != 1.0))[0]).tolist()\n",
    "#     ratio_df['Similar'][index] = sim_ind_list\n",
    "#     index+=1\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# num_rows = ratio_df.shape[0] \n",
    "# tot_tuples = []\n",
    "\n",
    "# final_cut_strings = [None] * num_rows\n",
    "# count_not_similar = 0  \n",
    "# not_sim_list = []\n",
    "# sim_list = []\n",
    "\n",
    "# for r in range(num_rows):\n",
    "#     if (len(ratio_df['Similar'][r]) != 0):\n",
    "#         sim_list.append(r)\n",
    "#         #print(\"has similar > 0.9 at index : \" + str(r))\n",
    "#         list_cut = []\n",
    "#         for ind in ratio_df['Similar'][r]:\n",
    "#             list_of_triples = SeqMatcher(None, ratio_df['Page'][r], ratio_df['Page'][ind]).get_matching_blocks()\n",
    "#             zeroth_triple = list_of_triples[0] #first triple, most likely in beginning, most likely a header\n",
    "#             n = zeroth_triple[2] #j to j+n are the indices of the overlapping part\n",
    "#             orig_string = ratio_df['Page'][ind]\n",
    "#             cut_down_string = orig_string[:zeroth_triple[1]] + orig_string[zeroth_triple[1] + n:] #removes overlapping part\n",
    "#             list_cut.append([ind, cut_down_string])\n",
    "#         tot_tuples.extend(list_cut) #list of tuples\n",
    "#     else:\n",
    "#         #print(\"no similar at index : \" + str(r))\n",
    "#         not_sim_list.append(r)\n",
    "#         count_not_similar +=1\n",
    "#         final_cut_strings[r]= ratio_df['Page'][r]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# list_grouped = [[] for x in range(num_rows)] #big list, just put list of strings in spots where needed\n",
    "# list_indices_of_groups = []\n",
    "# for ind in range(num_rows):\n",
    "#     for tup in tot_tuples:\n",
    "#         if(tup[0] == ind):\n",
    "#             list_grouped[ind].append(tup[1]) #attach that tuple's string\n",
    "#             list_indices_of_groups.append(tup[0])\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# #not used\n",
    "# unique_group_ind = set(list_indices_of_groups) \n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# ind_fill_final_grouped = []\n",
    "# i = 0\n",
    "# for group in list_grouped:\n",
    "#     if (len(group) != 0):\n",
    "#         #print(\"list ready to insert at index \" + str(i) + \"\\n\")\n",
    "#         ind_fill_final_grouped.append(i)\n",
    "    \n",
    "#     i+=1\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# spot = 0\n",
    "# #add into final cut strings, the new \"cut down\" versions of appropriate strings\n",
    "# for li in list_grouped:\n",
    "#     if (len(li) != 0):\n",
    "#         #print(unique_ind_list[spot])\n",
    "#         final_cut_strings[ind_fill_final_grouped[spot]]= min(li, key=len) #inserts into correct index, what was None before, add in that string now\n",
    "#         spot+=1\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def create_first_header_cut(tot_tuples, final_cut_strings, num_rows):\n",
    "    #num_rows = ratio_df.shape[0]\n",
    "    list_grouped = [[] for x in range(num_rows)] #big list, just put list of strings in spots where needed\n",
    "    #list_indices_of_groups = []\n",
    "    for ind in range(num_rows):\n",
    "        for tup in tot_tuples:\n",
    "            if(tup[0] == ind):\n",
    "                list_grouped[ind].append(tup[1]) #attach that tuple's string\n",
    "                #list_indices_of_groups.append(tup[0])\n",
    "                \n",
    "    ind_fill_final_grouped = []\n",
    "    i = 0\n",
    "    for group in list_grouped:\n",
    "        if (len(group) != 0):\n",
    "            ind_fill_final_grouped.append(i)\n",
    "    \n",
    "        i+=1\n",
    "        \n",
    "    spot = 0\n",
    "    #add into final cut strings, the new \"cut down\" versions of appropriate strings\n",
    "    for li in list_grouped:\n",
    "        if (len(li) != 0):\n",
    "            #print(unique_ind_list[spot])\n",
    "            final_cut_strings[ind_fill_final_grouped[spot]]= min(li, key=len) #inserts into correct index, what was None before, add in that string now\n",
    "            spot+=1\n",
    "    \n",
    "    return final_cut_strings\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# #first removal of headers in final_cut_Strings currently, but now we want to cut down headers more\n",
    "# #take out text before the first sentence or text before the first group of 7+ words\n",
    "\n",
    "# super_final_strings = []\n",
    "# for s in final_cut_strings:\n",
    "#     use_punc = False\n",
    "#     use_sev = False\n",
    "#     punc = [\",\", \".\", \":\", \";\"]\n",
    "#     p_list = []\n",
    "#     for p in punc:\n",
    "#         if (s.find(p) != -1):\n",
    "#             p_list.append(s.find(p))\n",
    "#         else:\n",
    "#             p_list.append(len(s))\n",
    "    \n",
    "#     punc_ind = min(p_list)\n",
    "    \n",
    "#     n_list = [index for index, k in enumerate(s) if k=='\\n']\n",
    "#     start_punc = len(s)\n",
    "#     for i in n_list:\n",
    "#         if(i < punc_ind):\n",
    "#             start_punc = i # start_punc equals the largest index of \\n that's less than index of first punctuation\n",
    "    \n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     total = \"\"\n",
    "#     list_totals = []\n",
    "#     st_en = []\n",
    "#     for c in s:\n",
    "#         if (c not in ['\\n', '\\t']):\n",
    "#             total+=(c)\n",
    "#             end+=1\n",
    "#         else :\n",
    "#             if(len(total.split()) >= 7): # we hit 7 words or more, wipe everything before start index\n",
    "#                 #list_totals.append(total)\n",
    "#                 st_en.append((start, end))\n",
    "    \n",
    "#             total= \"\"\n",
    "#             start = end\n",
    "#     start_sev = len(s)-1 #len(s)-1 #make it huge by default; if there's no group of 7, then start punc will be the smallest\n",
    "#     if len(st_en) > 0:\n",
    "#         start_sev = st_en[0][0] #index of first group of words that's >= 7 words; 0th tuple's start value\n",
    "   \n",
    "\n",
    "#     #take smaller of the two indices, since we want to use the property which occurs first\n",
    "#     if start_punc < start_sev:\n",
    "#         #if start of sentence which ends in/contains puncuation occurs earlier, wipe eveything before that index\n",
    "#         #only take that index +1 and on, start right after the new line\n",
    "#         new_string = s[start_punc+1:] \n",
    "#         super_final_strings.append(new_string)\n",
    "        \n",
    "#     else:\n",
    "#         #if start of group of words that >= 7 occurs earlier than a sentence with punctuation, wipe eveything before that index\n",
    "#         #only take that index and on, statr using that begining of the group of 7+ words\n",
    "#         new_string = s[start_sev:] \n",
    "#         super_final_strings.append(new_string)\n",
    "        \n",
    "     \n",
    "            \n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def create_second_header_cut(first_header_cut):\n",
    "    \n",
    "    super_final_strings = []\n",
    "    for s in first_header_cut:\n",
    "        s_new = \"\"\n",
    "        if(s is None):\n",
    "            s_new = \"\"\n",
    "        else:\n",
    "            s_new = s\n",
    "        use_punc = False\n",
    "        use_sev = False\n",
    "        punc = [\",\", \".\", \":\", \";\"]\n",
    "        p_list = []\n",
    "        for p in punc:\n",
    "            if (s_new.find(p) != -1):\n",
    "                p_list.append(s_new.find(p))\n",
    "            else:\n",
    "                p_list.append(len(s_new))\n",
    "\n",
    "        punc_ind = min(p_list)\n",
    "\n",
    "        n_list = [index for index, k in enumerate(s_new) if k=='\\n']\n",
    "        start_punc = len(s_new)\n",
    "        for i in n_list:\n",
    "            if(i < punc_ind):\n",
    "                start_punc = i # start_punc equals the largest index of \\n that's less than index of first punctuation\n",
    "\n",
    "        start = 0\n",
    "        end = 0\n",
    "        total = \"\"\n",
    "        list_totals = []\n",
    "        st_en = []\n",
    "        for c in s_new:\n",
    "            if (c not in ['\\n', '\\t']):\n",
    "                total+=(c)\n",
    "                end+=1\n",
    "            else :\n",
    "                if(len(total.split()) >= 7): # we hit 7 words or more, wipe everything before start index\n",
    "                    #list_totals.append(total)\n",
    "                    st_en.append((start, end))\n",
    "\n",
    "                total= \"\"\n",
    "                start = end\n",
    "        start_sev = len(s_new)-1 #len(s)-1 #make it huge by default; if there's no group of 7, then start punc will be the smallest\n",
    "        if len(st_en) > 0:\n",
    "            start_sev = st_en[0][0] #index of first group of words that's >= 7 words; 0th tuple's start value\n",
    "\n",
    "\n",
    "        #take smaller of the two indices, since we want to use the property which occurs first\n",
    "        if start_punc < start_sev:\n",
    "            #if start of sentence which ends in/contains puncuation occurs earlier, wipe eveything before that index\n",
    "            #only take that index +1 and on, start right after the new line\n",
    "            new_string = s_new[start_punc+1:] \n",
    "            super_final_strings.append(new_string)\n",
    "\n",
    "        else:\n",
    "            #if start of group of words that >= 7 occurs earlier than a sentence with punctuation, wipe eveything before that index\n",
    "            #only take that index and on, statr using that begining of the group of 7+ words\n",
    "            new_string = s_new[start_sev:] \n",
    "            super_final_strings.append(new_string)\n",
    "        \n",
    "    return super_final_strings\n",
    "\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "#compare between pages of each school\n",
    "\n",
    "def remove_string_overlaps(tuplist):\n",
    "   \n",
    "    unique_tuplist = []\n",
    "    seen_pages = set() # Initialize list of known pages for a school\n",
    "    unique_pages=[]\n",
    "    reversed_pages = []\n",
    "    tup_indices = []\n",
    "\n",
    "    cleaned_strings = []\n",
    "    new_list = []\n",
    "    \n",
    "    if (str(tuplist) == 'nan') or (len(tuplist) == 0):\n",
    "        return new_list\n",
    "    else :\n",
    "        \n",
    "        for tup in tuplist:\n",
    "            if (tup is not None):\n",
    "                seen_pages.add(tup[3])\n",
    "\n",
    "        for i in range(len(tuplist)):\n",
    "            #(tuplist[i][3] is not np.nan) and\n",
    "            if (tuplist[i][3] in seen_pages) and (tuplist[i][3]  not in unique_pages) and (tuplist[i][3] is not None):\n",
    "                unique_tuplist.append(tuplist[i])\n",
    "                unique_pages.append(tuplist[i][3])\n",
    "                reversed_pages.append(tuplist[i][3][::-1])\n",
    "                tup_indices.append(i)\n",
    "                #print(\"unique page : \" + str(i))\n",
    "\n",
    "        #now compare all pages with each other \n",
    "        #print(unique_tuplist)\n",
    "        tot_tuples, final_cut_strings, ratio_df = create_sim_df(unique_pages)\n",
    "        first_header_cut = create_first_header_cut(tot_tuples, final_cut_strings, ratio_df.shape[0])\n",
    "\n",
    "        #first removal of headers in final_cut_Strings currently, but now we want to cut down headers more\n",
    "        #take out text before the first sentence or text before the first group of 7+ words  \n",
    "        second_header_cut = create_second_header_cut(first_header_cut)\n",
    "\n",
    "\n",
    "        #now run process on reversed strings\n",
    "\n",
    "        rev_tot_tuples, rev_final_cut_strings, rev_ratio_df = create_sim_df(reversed_pages)\n",
    "\n",
    "        rev_first_header_cut = create_first_header_cut(rev_tot_tuples, rev_final_cut_strings, rev_ratio_df.shape[0])\n",
    "\n",
    "        for i in range(len(rev_first_header_cut)):\n",
    "            #find where to cut the footer off , index\n",
    "\n",
    "            add_string = second_header_cut[i]\n",
    "            if i in tup_indices: \n",
    "                if(rev_first_header_cut is None):\n",
    "                    rev_first_header_cut = \"\"\n",
    "                if(rev_first_header_cut[i] is None):\n",
    "                    rev_first_header_cut[i] = \"\"\n",
    "                forward_string = rev_first_header_cut[i][::-1]\n",
    "                #print(forward_string)\n",
    "                sept = int(len(forward_string)/2)\n",
    "                half_string = forward_string[len(forward_string) - sept:]\n",
    "                #find that half in the regular string, and get the end of the half\n",
    "                #print(type(second_header_cut[i]))\n",
    "                #print(type(half_string))\n",
    "                end_index = (second_header_cut[i]).find(half_string) + len(half_string) - 1\n",
    "                add_string = second_header_cut[i][:end_index]\n",
    "                #remove the footer aka remove stuff after the end_index\n",
    "                #keep the stuff before end_index\n",
    "\n",
    "            cleaned_strings.append(add_string)\n",
    "\n",
    "\n",
    "        #then iterate through cleaned_strings and inset into each tuple\n",
    "\n",
    "        for count in range(len(cleaned_strings)):\n",
    "            new_tup = (tuplist[tup_indices[count]][0], tuplist[tup_indices[count]][1], tuplist[tup_indices[count]][2], cleaned_strings[count])\n",
    "            new_list.append(new_tup)\n",
    "    #         print(tup_indices[count])\n",
    "    #         print(cleaned_strings[count])\n",
    "    #         print(\"\\n  \\n\")\n",
    "\n",
    "    #     print(len(new_list))\n",
    "    #     print(len(tup_indices))\n",
    "        return new_list\n",
    "\n",
    "\n",
    "#first make a list of tuples that has no nan values and has\n",
    "#if not np.isnan(tuplist):   CHECK FOR NAN OUTSIDE\n",
    "\n",
    "#k = remove_string_overlaps(new_data['WEBTEXT'][11])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# index = 0\n",
    "# ki = []\n",
    "# lengths = []\n",
    "# new_data['WEBTEXT'] = new_data['WEBTEXT'].fillna(\"\")\n",
    "# for li in new_data['WEBTEXT']:\n",
    "#     if len(li) <100:\n",
    "#         ki.append(index)\n",
    "        \n",
    "#         #print(index)\n",
    "#     lengths.append(len(li))\n",
    "#     index+=1\n",
    "# print(len(ki))\n",
    "# print(len(new_data['WEBTEXT']))\n",
    "# print(np.average(lengths))\n",
    "# print(np.median(lengths))\n",
    "# ki\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# lengths\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# print(set(lengths))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# twenty_count = 0\n",
    "# for li in new_data['WEBTEXT']:\n",
    "#     if len(li) <=20:\n",
    "#         twenty_count +=1\n",
    "\n",
    "# print(twenty_count/len(new_data['WEBTEXT']))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# k = remove_string_overlaps(new_data['WEBTEXT'][9])\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# k\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# r = sorted(lengths, key=int)  \n",
    "# np.median(r)\n",
    "# for i in r:\n",
    "#     print(i)\n",
    "\n",
    "\n",
    "# In[36]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply remove_string_overlaps on each school, aka on each row of new_data\n",
    "#since pages of a school will likely be similar to the other pages within that school own\n",
    "\n",
    "def parse_df(old_list):   \n",
    "    new_list = remove_string_overlaps(old_list)\n",
    "    return new_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[37]:\n",
    "\n",
    "\n",
    "new_data['WEBTEXT'] = new_data['WEBTEXT'].fillna(\"\")\n",
    "\n",
    "arr_of_dfs = np.array_split(new_data, len(new_data['WEBTEXT']))\n",
    "\n",
    "global merged_df_file\n",
    "merged_df_file = folder_prefix+\"merged_df_WEBTEXT.csv\" # Prepare file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[38]:\n",
    "\n",
    "\n",
    "# tqdm.pandas(desc=\"Processing:\")\n",
    "\n",
    "# arr_of_dfs[0]['WEBTEXT'] = arr_of_dfs[0]['WEBTEXT'].progress_apply(parse_df)\n",
    "\n",
    "# arr_of_dfs[0].to_csv(merged_df_file, mode=\"w\", index=False, header=arr_of_dfs[0].columns.values, sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "#df = pd.read_csv(folder_prefix+'_mergedf_WEBTEXT.csv', header=arr_of_dfs[0].columns.values, sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# In[43]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[45]:\n",
    "\n",
    "\n",
    "def chunk_assign(df_chunk):\n",
    "    global num\n",
    "    \n",
    "    need_clean_chunk = df_chunk.loc[df_chunk['OVERLAPS_REMOVED'] == 0]\n",
    "    already_cleaned_chunk = df_chunk.loc[df_chunk['OVERLAPS_REMOVED'] == 1]\n",
    "    if (need_clean_chunk.shape[0] > 0):\n",
    "        need_clean_chunk['WEBTEXT'] = need_clean_chunk['WEBTEXT'].apply(parse_df)\n",
    "        need_clean_chunk['OVERLAPS_REMOVED'] = 1\n",
    "        final_chunk = pd.concat([need_clean_chunk, already_cleaned_chunk])\n",
    "    else:\n",
    "        final_chunk = pd.concat([already_cleaned_chunk, need_clean_chunk])\n",
    "\n",
    "    \n",
    "    \n",
    "    if num==0: # Save first slice to new file (overwriting if needed)\n",
    "        final_chunk.to_csv(folder_prefix + \"second_df_WEBTEXT.csv\", mode=\"w\", index=False, header=df_chunk.columns.values, sep=\"\\t\", encoding=\"utf-8\")\n",
    "        \n",
    "    else:\n",
    "        final_chunk.to_csv(folder_prefix + \"second_df_WEBTEXT.csv\", mode=\"a\", index=False, header=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "    \n",
    "#     curr_final_df = pd.read_csv(merged_df_file , sep=\"\\t\", low_memory=False, encoding=\"utf-8\")\n",
    "#     print()\n",
    "    \n",
    "    num+=1\n",
    "    \n",
    "    #logging.info((need_clean_chunk.shape[0] / df_chunk.shape[0]) * 100)\n",
    "    #print(num * 44)\n",
    "    \n",
    "    #free chunk?\n",
    "   # logging.info(\"df chunk saved to \" + df_filepath )\n",
    "    \n",
    "    return df_chunk\n",
    "\n",
    "\n",
    "# In[46]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "#start_time = time.time()\n",
    "#tqdm.pandas(desc=\"Processing:\")\n",
    "#tqdm.pandas(desc=\"Processing all: \" )\n",
    "orig_num_rows = new_data.shape[0] \n",
    "\n",
    "curr_merged_df = pd.read_csv(merged_df_file , sep=\"\\t\", low_memory=False, encoding=\"utf-8\")\n",
    "merged_num_rows = curr_merged_df.shape[0]\n",
    "\n",
    "numcpus = len(os.sched_getaffinity(0)) # Detect and assign number of available CPUs\n",
    "p = mp.Pool(numcpus)\n",
    "result_df = p.map(chunk_assign, arr_of_dfs)\n",
    "\n",
    "starttime=time.time()\n",
    "\n",
    "while True:   \n",
    "    second_df = pd.read_csv(folder_prefix + \"second_df_WEBTEXT.csv\", sep=\"\\t\", low_memory=False, encoding=\"utf-8\")\n",
    "    second_num_rows = second_df.shape[0]\n",
    "    sum_so_far = merged_num_rows + second_num_rows\n",
    "    diff = orig_num_rows - sum_so_far\n",
    "    logging.info(sum_so_far)\n",
    "    print(\"Total first + second num rows : \" + sum_so_far +  \" . Num rows to go : \" + diff)\n",
    "    time.sleep(300.0 - ((time.time() - starttime) % 300.0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "\n",
    "# #list of common words in footers\n",
    "# footer_list = [\"Copyright\", \"All Rights Reserved\",  \"Read More\", \n",
    "#                \"Useful Links\", \"Search\", \"Survey\", \"Feed\", \"Fax\", \"Address\",  \"Sitemap\", \n",
    "#               \"Jobs\"]\n",
    "# #facebook, contact us, enroll etc occurs in headers as well, so had to take that out\n",
    "\n",
    "# footers_removed_strings = []\n",
    "\n",
    "# #look for the keyword in each string and if found, remove all the text after it\n",
    "# for s in super_final_strings:\n",
    "#     no_newline = s.replace(\"\\n\", \" \")\n",
    "#     new_list = []\n",
    "#     for word in footer_list:\n",
    "#         if (no_newline.find(word) != -1):\n",
    "#             new_list.append(no_newline.find(word))\n",
    "#         else :\n",
    "#             new_list.append(len(s))\n",
    "\n",
    "#     #get the index of the earliest occurence of a keyword\n",
    "#     f_ind = min(new_list)\n",
    "    \n",
    "#     #go back to new line or period right before and wipe out everything after that\n",
    "#     n_list = [index for index, k in enumerate(s) if k in ['\\n', '.']]\n",
    "#     start_punc = len(s)\n",
    "#     for i in n_list:\n",
    "#         if(i < f_ind):\n",
    "#             start_punc = i # start_punc equals the largest index of \\n or . that's less than index of the keyword\n",
    "            \n",
    "#     if start_punc < f_ind:\n",
    "#             footer_rem = s[:start_punc]\n",
    "#             footers_removed_strings.append(footer_rem) \n",
    "#     else:\n",
    "#             footer_rem = s[:f_ind]\n",
    "#             footers_removed_strings.append(footer_rem) \n",
    "              \n",
    "\n",
    "\n",
    "# In[114]:\n",
    "\n",
    "\n",
    "# kr = super_final_strings[4]\n",
    "# no_newline = kr.replace(\"\\n\", \" \")\n",
    "# new_list = []\n",
    "# li = []\n",
    "# for word in footer_list:\n",
    "    \n",
    "#     if (no_newline.find(word) != -1):\n",
    "#         new_list.append(no_newline.find(word)) #index at which the word starts\n",
    "#     else :\n",
    "#         new_list.append(len(kr))\n",
    "\n",
    "# #get the index of the earliest occurence of a keyword\n",
    "# f_ind = min(new_list)\n",
    "    \n",
    "# #go back to new line or period right before and wipe out everything after that\n",
    "# n_list = [index for index, k in enumerate(kr) if k in ['\\n', '.']]\n",
    "# start_punc = len(kr)\n",
    "# for i in n_list:\n",
    "#     if(i < f_ind):\n",
    "#         start_punc = i # start_punc equals the largest index of \\n or . that's less than index of the keyword\n",
    "            \n",
    "# if start_punc < f_ind: #if punctuation occurs before keyword, wipe out everything after punctuation\n",
    "#         footer_rem = kr[:start_punc]\n",
    "#         li.append(footer_rem) \n",
    "# else:\n",
    "#         footer_rem = kr[:f_ind] #else just wipe out everything after punctuation\n",
    "#         li.append(footer_rem) \n",
    "# print(word + \": \" + \"start_punc : \" + str(start_punc) + \" . f_ind : \" + str(f_ind))\n",
    "\n",
    "# li[0]\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "\n",
    "#kr[646:]\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "#footers_removed_strings are the final, most cut down versions of the web text\n",
    "#we did process on \"WEBTEXT\" column of new_data\n",
    "#now repeat for \"CMO_WEBTEXT\" column of new_data\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# footers_removed_strings\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# for count in range(len(footers_removed_strings)):\n",
    "#     if \"\\nPre-enroll\" in footers_removed_strings[len(footers_removed_strings[count])-13:]:\n",
    "#         print(str(count))\n",
    "        \n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#ratio_df['Page'][4]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#footers_removed_strings[1]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "#super_final_strings[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
