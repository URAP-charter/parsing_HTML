{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: bz2file in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.92 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.92->boto3->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.92->boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: singledispatch in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence cleaning preliminaries complete...\n"
     ]
    }
   ],
   "source": [
    "# For loading functions from files in data_tools directory:\n",
    "import sys; sys.path.insert(0, \"../parsing\")\n",
    "\n",
    "# ## Create lists of stopwords, punctuation, and unicode characters\n",
    "from clean_functions import clean_sentence, stopwords_make, punctstr_make, unicode_make\n",
    "import clean_functions\n",
    "\n",
    "print(\"Sentence cleaning preliminaries complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8\n",
    "\n",
    "# Word Embedding Models: Preprocessing and Model Training\n",
    "# Project title: Charter school identities \n",
    "# Creator: Jaren Haber, PhD Candidate\n",
    "# Institution: Department of Sociology, University of California, Berkeley\n",
    "# Date created: July 20, 2018\n",
    "# Date last edited: November 8, 2018\n",
    "\n",
    "# ## Initialize\n",
    "\n",
    "# Import general packages\n",
    "import imp, importlib # For working with modules\n",
    "import nltk # for natural language processing tools\n",
    "import pandas as pd # for working with dataframes\n",
    "#from pandas.core.groupby.groupby import PanelGroupBy # For debugging\n",
    "import numpy as np # for working with numbers\n",
    "import pickle # For working with .pkl files\n",
    "from tqdm import tqdm # Shows progress over iterations, including in pandas via \"progress_apply\"\n",
    "import sys # For terminal tricks\n",
    "import _pickle as cPickle # Optimized version of pickle\n",
    "import gc # For managing garbage collector\n",
    "import timeit # For counting time taken for a process\n",
    "import datetime # For working with dates & times\n",
    "\n",
    "# Import packages for cleaning, tokenizing, and stemming text\n",
    "import re # For parsing text\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words (it just cuts off the ends)\n",
    "from nltk.stem.porter import PorterStemmer # approximate but effective (and common) method of normalizing words: stems words by implementing a hierarchy of linguistic rules that transform or cut off word endings\n",
    "stem = PorterStemmer().stem # Makes stemming more accessible\n",
    "from nltk.corpus import stopwords # for eliminating stop words\n",
    "import gensim # For word embedding models\n",
    "from gensim.models.phrases import Phrases # Makes word2vec more robust: Looks not just at  To look for multi-word phrases within word2vec\n",
    "\n",
    "# Import packages for multiprocessing\n",
    "import os # For navigation\n",
    "numcpus = len(os.sched_getaffinity(0)) # Detect and assign number of available CPUs\n",
    "from multiprocessing import Pool # key function for multiprocessing, to increase processing speed\n",
    "pool = Pool(processes=numcpus) # Pre-load number of CPUs into pool function\n",
    "import Cython # For parallelizing word2vec\n",
    "mpdo = False # Set to 'True' if using multiprocessing--faster for creating words by sentence file, but more complicated\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing sentence data detected at /home/jovyan/work/Charter-school-identities/data/wem_wordsent_data_train250_nostem_unlapped_clean2.pkl, skipping preprocessing sentences.\n",
      "Existing sentence + phrase data detected at /home/jovyan/work/Charter-school-identities/data/wem_phrasesent_data_train250_nostem_unlapped_clean2.pkl, skipping preprocessing sentence phrases.\n",
      "Sentence cleaning preliminaries complete...\n"
     ]
    }
   ],
   "source": [
    "# ## Prepare to read data\n",
    "\n",
    "# Define file paths\n",
    "if mpdo:\n",
    "    wordsent_path = \"../../Charter-school-identities/data/wem_wordsent_data_train250_nostem_unlapped_clean2.txt\"\n",
    "else:\n",
    "    wordsent_path = \"../../Charter-school-identities/data/wem_wordsent_data_train250_nostem_unlapped_clean2.pkl\"\n",
    "charters_path = \"../../nowdata/traincf_2015.pkl\" # All text data; only charter schools (regardless if open or not)\n",
    "phrasesent_path = \"../../Charter-school-identities/data/wem_phrasesent_data_train250_nostem_unlapped_clean2.pkl\"\n",
    "#wemdata_path = \"../data/wem_data.pkl\"\n",
    "model_path = \"../../Charter-school-identities/data/wem_model_train250_nostem_unlapped_300d_clean2.bin\"\n",
    "vocab_path = \"../../Charter-school-identities/data/wem_vocab_train250_nostem_unlapped_300d_clean2.txt\"\n",
    "vocab_path_old = \"../../Charter-school-identities/data/wem_vocab_train250_nostem_unlapped_300d_clean.txt\"\n",
    "\n",
    "# Check if sentences data already exists, to save time\n",
    "try:\n",
    "    if (os.path.exists(wordsent_path)) and (os.path.getsize(wordsent_path) > 10240): # Check if file not empty (at least 10K)\n",
    "        print(\"Existing sentence data detected at \" + str(os.path.abspath(wordsent_path)) + \", skipping preprocessing sentences.\")\n",
    "        sented = True\n",
    "    else:\n",
    "        sented = False\n",
    "except FileNotFoundError or OSError: # Handle common errors when calling os.path.getsize() on non-existent files\n",
    "    sented = False\n",
    "\n",
    "# Check if sentence phrases data already exists, to save time\n",
    "try:\n",
    "    if (os.path.exists(phrasesent_path)) and (os.path.getsize(phrasesent_path) > 10240): # Check if file not empty (at least 10K)\n",
    "        print(\"Existing sentence + phrase data detected at \" + str(os.path.abspath(phrasesent_path)) + \", skipping preprocessing sentence phrases.\")\n",
    "        phrased = True\n",
    "    else:\n",
    "        phrased = False\n",
    "except FileNotFoundError or OSError: # Handle common errors when calling os.path.getsize() on non-existent files\n",
    "    phrased = False\n",
    "    \n",
    "    \n",
    "# ## Create lists of stopwords, punctuation, and unicode characters\n",
    "\n",
    "# Create stopwords list\n",
    "stop_word_list = list(set(stopwords.words(\"english\"))) # list of english stopwords\n",
    "\n",
    "# Add dates to stopwords\n",
    "for i in range(1,13):\n",
    "    stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "for i in range(1,13):\n",
    "    stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "for i in range(1, 2100):\n",
    "    stop_word_list.append(str(i))\n",
    "\n",
    "# Add other common stopwords\n",
    "stop_word_list.append('00') \n",
    "stop_word_list.extend(['mr', 'mrs', 'sa', 'fax', 'email', 'phone', 'am', 'pm', 'org', 'com', \n",
    "                       'Menu', 'Contact Us', 'Facebook', 'Calendar', 'Lunch', 'Breakfast', 'FAQs', 'FAQ']) # web stopwords\n",
    "stop_word_list.extend(['el', 'en', 'la', 'los', 'para', 'las', 'san']) # Spanish stopwords\n",
    "stop_word_list.extend(['angeles', 'diego', 'harlem', 'bronx', 'austin', 'antonio']) # cities with many charter schools\n",
    "\n",
    "# Add state names & abbreviations (both uppercase and lowercase) to stopwords\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \n",
    "          \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \n",
    "          \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \n",
    "          \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \n",
    "          \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WI\", \"WV\", \"WY\", \n",
    "          \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \n",
    "          \"Colorado\", \"Connecticut\", \"District of Columbia\", \"Delaware\", \"Florida\", \n",
    "          \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \n",
    "          \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \n",
    "          \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \n",
    "          \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n",
    "          \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \n",
    "          \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \n",
    "          \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \n",
    "          \"Vermont\", \"Virginia\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
    "for state in states:\n",
    "    stop_word_list.append(state)\n",
    "for state in [state.lower() for state in states]:\n",
    "    stop_word_list.append(state)\n",
    "\n",
    "# Add to stopwords useless and hard-to-formalize words/chars from first chunk of previous model vocab (e.g., a3d0, \\fs19)\n",
    "# First create whitelist of useful terms probably in that list, explicitly exclude from junk words list both these and words with underscores (common phrases)\n",
    "whitelist = [\"Pre-K\", \"pre-k\", \"pre-K\", \"preK\", \"prek\", \n",
    "             \"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\", \"12th\", \n",
    "             \"1st-grade\", \"2nd-grade\", \"3rd-grade\", \"4th-grade\", \"5th-grade\", \"6th-grade\", \n",
    "             \"7th-grade\", \"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\", \"12th-grade\", \n",
    "             \"1st-grader\", \"2nd-grader\", \"3rd-grader\", \"4th-grader\", \"5th-grader\", \"6th-grader\", \n",
    "             \"7th-grader\", \"8th-grader\", \"9th-grader\", \"10th-grader\", \"11th-grader\", \"12th-grader\", \n",
    "             \"1stgrade\", \"2ndgrade\", \"3rdgrade\", \"4thgrade\", \"5thgrade\", \"6thgrade\", \n",
    "             \"7thgrade\", \"8thgrade\", \"9thgrade\", \"10thgrade\", \"11thgrade\", \"12thgrade\", \n",
    "             \"1stgrader\", \"2ndgrader\", \"3rdgrader\", \"4thgrader\", \"5thgrader\", \"6thgrader\", \n",
    "             \"7thgrader\", \"8thgrader\", \"9thgrader\", \"10thgrader\", \"11thgrader\", \"12thgrader\"]\n",
    "with open(vocab_path_old) as f: # Load vocab from previous model\n",
    "    junk_words = f.read().splitlines() \n",
    "junk_words = [word for word in junk_words[:8511] if ((not \"_\" in word) \n",
    "                                                     and (not any(term in word for term in whitelist)))]\n",
    "stop_word_list.extend(junk_words)\n",
    "    \n",
    "# Create punctuations list\n",
    "import string # for one method of eliminating punctuation\n",
    "punctuations = list(string.punctuation) # assign list of common punctuation symbols\n",
    "#addpuncts = ['*','•','©','–','`','’','“','”','»','.','×','|','_','§','…','⎫'] # a few more punctuations also common in web text\n",
    "#punctuations += addpuncts # Expand punctuations list\n",
    "#punctuations = list(set(punctuations)) # Remove duplicates\n",
    "punctuations.remove('-') # Don't remove hyphens - dashes at beginning and end of words are handled separately)\n",
    "punctuations.remove(\"'\") # Don't remove possessive apostrophes - those at beginning and end of words are handled separately\n",
    "punctstr = \"\".join([char for char in punctuations]) # Turn into string for regex later\n",
    "\n",
    "# Create list of unicode characters\n",
    "unicode_list  = []\n",
    "for i in range(1000,3000):\n",
    "    unicode_list.append(chr(i))\n",
    "unicode_list.append(\"_cid:10\") # Common in webtext junk\n",
    "\n",
    "print(\"Sentence cleaning preliminaries complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Define helper functions\n",
    "\n",
    "def quickpickle_load(picklepath):\n",
    "    '''Very time-efficient way to load pickle-formatted objects into Python.\n",
    "    Uses C-based pickle (cPickle) and gc workarounds to facilitate speed. \n",
    "    Input: Filepath to pickled (*.pkl) object.\n",
    "    Output: Python object (probably a list of sentences or something similar).'''\n",
    "\n",
    "    with open(picklepath, 'rb') as loadfile:\n",
    "        \n",
    "        gc.disable() # disable garbage collector\n",
    "        outputvar = cPickle.load(loadfile) # Load from picklepath into outputvar\n",
    "        gc.enable() # enable garbage collector again\n",
    "    \n",
    "    return outputvar\n",
    "def quickpickle_dump(dumpvar, picklepath):\n",
    "    '''Very time-efficient way to dump pickle-formatted objects from Python.\n",
    "    Uses C-based pickle (cPickle) and gc workarounds to facilitate speed. \n",
    "    Input: Python object (probably a list of sentences or something similar).\n",
    "    Output: Filepath to pickled (*.pkl) object.'''\n",
    "\n",
    "    with open(picklepath, 'wb') as destfile:\n",
    "        \n",
    "        gc.disable() # disable garbage collector\n",
    "        cPickle.dump(dumpvar, destfile) # Dump dumpvar to picklepath\n",
    "        gc.enable() # enable garbage collector again\n",
    "    \n",
    "    \n",
    "def write_list(file_path, textlist):\n",
    "    \"\"\"Writes textlist to file_path. Useful for recording output of parse_school().\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        \n",
    "        for elem in textlist:\n",
    "            file_handler.write(\"{}\\n\".format(elem))\n",
    "    \n",
    "    return    \n",
    "\n",
    "\n",
    "def load_list(file_path):\n",
    "    \"\"\"Loads list into memory. Must be assigned to object.\"\"\"\n",
    "    \n",
    "    textlist = []\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            textlist.append(line)\n",
    "            line = file_handler.readline()\n",
    "    return textlist\n",
    "    \n",
    "    \n",
    "def write_sentence(sentence, file_path):\n",
    "    \"\"\"Writes sentence to file at file_path.\n",
    "    Useful for recording first row's output of preprocess_wem() one sentence at a time.\n",
    "    Input: Sentence (list of strings), path to file to save it\n",
    "    Output: Nothing (saves to disk)\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w+') as file_handler:\n",
    "        for word in sentence: # Iterate over words in sentence\n",
    "            if word == \"\":\n",
    "                pass\n",
    "            else:\n",
    "                file_handler.write(word + \" \") # Write each word on same line, followed by space\n",
    "            \n",
    "        file_handler.write(\"\\n\") # After sentence is fully written, close line (by inserting newline)\n",
    "            \n",
    "    return\n",
    "\n",
    "\n",
    "def append_sentence(sentence, file_path):\n",
    "    \"\"\"Appends sentence to file at file_path. \n",
    "    Useful for recording each row's output of preprocess_wem() one sentence at a time.\n",
    "    Input: Sentence (list of strings), path to file to save it\n",
    "    Output: Nothing (saves to disk)\"\"\"\n",
    "\n",
    "    with open(file_path, 'a+') as file_handler:\n",
    "        for word in sentence: # Iterate over words in sentence\n",
    "            if word == \"\":\n",
    "                pass\n",
    "            else:\n",
    "                file_handler.write(word + \" \") # Write each word on same line, followed by space\n",
    "            \n",
    "        file_handler.write(\"\\n\") # After sentence is fully written, close line (by inserting newline)\n",
    "            \n",
    "    return\n",
    "\n",
    "    \n",
    "def load_tokslist(file_path):\n",
    "    \"\"\"Loads from file and word-tokenizes list of \"\\n\"-separated, possibly multi-word strings (i.e., sentences). \n",
    "    Output must be assigned to object.\n",
    "    Input: Path to file with list of strings\n",
    "    Output: List of word-tokenized strings, i.e. sentences\"\"\"\n",
    "    \n",
    "    textlist = []\n",
    "    \n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline() # Read first line\n",
    "        \n",
    "        while line: # Continue while there's still a line to read\n",
    "            textlist.append(word for word in word_tokenize(line)) # Tokenize each line by word while loading in\n",
    "            line = file_handler.readline() # Read next line\n",
    "            \n",
    "    return textlist\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    \"\"\"Removes numbers, emails, URLs, unicode characters, hex characters, and punctuation from a sentence \n",
    "    separated by whitespaces. Returns a tokenized, cleaned list of words from the sentence.\n",
    "    \n",
    "    Args: \n",
    "        Sentence, i.e. string that possibly includes spaces and punctuation\n",
    "    Returns: \n",
    "        Cleaned & tokenized sentence, i.e. a list of cleaned, lower-case, one-word strings\"\"\"\n",
    "    \n",
    "    global unicode_list, punctstr, stop_word_list # Access useful lists\n",
    "    \n",
    "    # Replace unicode spaces, tabs, and underscores with spaces, and remove whitespaces from start/end of sentence:\n",
    "    sentence = sentence.replace(u\"\\xa0\", u\" \").replace(u\"\\\\t\", u\" \").replace(u\"_\", u\" \").strip(\" \")\n",
    "    \n",
    "    # Remove hex characters (e.g., \\xa0\\, \\x80):\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]', r'', sentence) #replace anything that starts with a hex character \n",
    "\n",
    "    # Replace \\\\x, \\\\u, \\\\b, or anything that ends with \\u2605\n",
    "    sentence = re.sub(r\"\\\\x.*|\\\\u.*|\\\\b.*|\\u2605$\", \"\", sentence)\n",
    "        \n",
    "    # Remove all elements that appear in unicode_list (looks like r'u1000|u10001|'):\n",
    "    sentence = re.sub(r'|'.join(map(re.escape, unicode_list)), '', sentence)\n",
    "    \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence) # Remove numbers\n",
    "    \n",
    "    sent_list = [] # Initialize empty list to hold tokenized sentence (words added one at a time)\n",
    "    \n",
    "    for word in sentence.split(): # Split by spaces and iterate over words\n",
    "        \n",
    "        word = word.strip() # Remove leading and trailing spaces\n",
    "        \n",
    "        # Filter out emails and URLs:\n",
    "        if (\"@\" not in word and not word.startswith(('http', 'https', 'www', \"//\", \"\\\\\", 'x_', 'x/', 'srcimage', '\\\\')) and not word.endswith(('.com', '.net', '.gov', '.org', '.jpg', '.pdf', 'png', 'jpeg', 'php'))):\n",
    "            \n",
    "            # Remove punctuation (only after URLs removed):\n",
    "            word = re.sub(r\"[\"+punctstr+\"]+\", r'', word).strip(\"'\").strip(\"-\") # Remove punctuations, and remove dashes and apostrophes only from start/end of words\n",
    "            if word not in stop_word_list: # Filter out stop words\n",
    "                \n",
    "                sent_list.append(word.lower()) # Add lower-cased word to list\n",
    "\n",
    "    return sent_list # Return clean, tokenized sentence\n",
    "\n",
    "\n",
    "def preprocess_wem(tuplist): # inputs were formerly: (tuplist, start, limit)\n",
    "    \n",
    "    '''This function cleans and tokenizes sentences, removing punctuation and numbers and making words into lower-case stems.\n",
    "    Inputs: list of four-element tuples, the last element of which holds the long string of text we care about;\n",
    "        an integer limit (bypassed when set to -1) indicating the DF row index on which to stop the function (for testing purposes),\n",
    "        and similarly, an integer start (bypassed when set to -1) indicating the DF row index on which to start the function (for testing purposes).\n",
    "    This function loops over five nested levels, which from high to low are: row, tuple, chunk, sentence, word.\n",
    "    Note: This approach maintains accurate semantic distances by keeping stopwords.'''\n",
    "        \n",
    "    global mpdo # Check if we're doing multiprocessing. If so, then mpdo=True\n",
    "    global words_by_sentence # Grants access to variable holding a list of lists of words, where each list of words represents a sentence in its original order (only relevant for this function if we're not using multiprocessing)\n",
    "    global pcount # Grants access to preprocessing counter\n",
    "    \n",
    "    known_pages = set() # Initialize list of known pages for a school\n",
    "\n",
    "    if type(tuplist)==float:\n",
    "        return # Can't iterate over floats, so exit\n",
    "    \n",
    "    #print('Parsing school #' + str(pcount)) # Print number of school being parsed\n",
    "\n",
    "    for tup in tuplist: # Iterate over tuples in tuplist (list of tuples)\n",
    "        if tup[3] in known_pages or tup=='': # Could use hashing to speed up comparison: hashlib.sha224(tup[3].encode()).hexdigest()\n",
    "            continue # Skip this page if exactly the same as a previous page on this school's website\n",
    "\n",
    "        for chunk in tup[3].split('\\n'): \n",
    "            for sent in sent_tokenize(chunk): # Tokenize chunk by sentences (in case >1 sentence in chunk)\n",
    "                sent = clean_sentence(sent) # Clean and tokenize sentence\n",
    "                \n",
    "                if ((sent == []) or (len(sent) == 0)): # If sentence is empty, continue to next sentence without appending\n",
    "                    continue\n",
    "                \n",
    "                # Save preprocessing sentence to file (if multiprocessing) or to object (if not multiprocessing)\n",
    "                if mpdo:\n",
    "                    try: \n",
    "                        if (os.path.exists(wordsent_path)) and (os.path.getsize(wordsent_path) > 0): \n",
    "                            append_sentence(sent, wordsent_path) # If file not empty, add to end of file\n",
    "                        else:\n",
    "                            write_sentence(sent, wordsent_path) # If file doesn't exist or is empty, start file\n",
    "                    except FileNotFoundError or OSError: # Handle common errors when calling os.path functions on non-existent files\n",
    "                        write_sentence(sent, wordsent_path) # Start file\n",
    "                \n",
    "                else:\n",
    "                    words_by_sentence.append(sent) # If not multiprocessing, just add sent to object\n",
    "                    \n",
    "                    \n",
    "        known_pages.add(tup[3])\n",
    "    \n",
    "    pcount += 1 # Add to counter\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF loaded from ../../nowdata/traincf_2015.pkl...\n",
      "Sample of the first 150 sentences:\n",
      "[['enroll_today', 'begin', 'path', 'graduation'], ['enroll', 'here'], ['uintah_river', 'high', 'school'], ['like', 'high', 'school', 'offers_unique', 'educational', 'experiences', 'students', 'teachers'], ['located', 'northern', 'ute', 'indian_reservation', 'urhs', 'immersed', 'native_american', 'culture'], ['we', 'value'], ['teacherstudent_relationships', 'make', 'efforts', 'provide', 'many', 'opportunities', 'teachers', 'students', 'build', 'positive_relationships', 'classroom'], ['teachers', 'students'], ['strive', 'exemplify', 'kindness', 'honesty', 'wisdom'], ['uintah_river', 'high', 'school', 'urhs', 'public', 'charter', 'school', 'located', 'uintah', 'ouray', 'reservation', 'roosevelt', 'vernal'], ['urhs', 'one', 'first', 'charter', 'schools', 'starting'], ['as', 'urhs', 'found', 'ute', 'indian_tribes', 'reservation', 'created', 'support', 'provide', 'needs', 'native', 'children', 'strong', 'positive_influence', 'native_american', 'culture'], ['urhs', 'alternative', 'school'], ['as', 'position', 'everything', 'power', 'help', 'students', 'track', 'graduation'], ['we', 'offer', 'small_class', 'sizes', 'plenty', 'opportunity', 'one', 'one', 'support'], ['over', 'years', 'urhs', 'cultivated', 'strong', 'tradition', 'student', 'athletics'], ['we', 'recently', 'taken', 'state', 'charter', 'league', 'boys_basketball', 'placed', 'state', 'boys_basketball', 'girls_volleyball', 'hope', 'place', 'state', 'basketball_season'], ['urhs', 'welcomes', 'students', 'backgrounds', 'ethnicity'], ['uintah_river', 'high', 'school'], ['s', 'e', 'fort_duchesne'], [], ['office'], ['principal', 'ben', 'pugh'], ['gym', 'location'], [], ['uintah_river', 'high', 'school'], ['fort_duchesne', 'gym'], ['uintah_river', 'high', 'school', 's', 'e', 'fort_duchesne'], ['copyright'], ['uintah_river', 'high', 'school', 's', 'e', 'fort_duchesne'], ['copyright'], ['address'], ['e', 's', 'fort_duchesne'], ['phone'], ['school', 'hours', 'ap', 'mondaythursday', 'unless_otherwise', 'noted'], ['social_media'], ['follow'], ['uintah_river', 'high', 'schoo'], ['erdirector'], ['email', 'tracie'], ['history', 'teacher', 'vacant'], ['if', 'would_like', 'apply', 'position', 'please'], ['click', 'here'], ['find', 'job', 'posted'], ['click', 'here'], ['application'], ['support', 'staff'], ['kea', 'tarnes'], ['jom', 'tutormentor'], ['email', 'kea'], ['eveningstar', 'curry'], ['jom', 'tutormentor'], ['email', 'eveningstar'], ['iktomi', 'favel'], ['jom', 'tutormentor'], ['iss', 'facilitator'], ['email', 'iktomi'], ['misty', 'mcclure'], ['tutormentor'], ['data', 'assistant'], ['email', 'misty'], ['sherri', 'street'], ['sped', 'tutormentor'], ['email', 'sherri'], ['ann', 'brock'], ['hall', 'monitor'], ['email', 'ann'], ['uintah_river', 'high', 'school', 's', 'e', 'fort_duchesne'], ['copyright'], [], ['a', 'federal', 'allocation_funds', 'schools', 'classified', 'low_income', 'purpose', 'assisting', 'students', 'demonstrate_proficiency', 'related', 'states', 'academic', 'standards'], ['what', 'purpose', 'title', 'i'], ['improve', 'school', 'impact', 'students', 'teachers', 'parents'], ['comprehensive', 'school', 'wide'], ['use', 'title', 'i', 'funds'], ['implement', 'school', 'goals_objectives'], ['attendance'], ['improve', 'reading', 'math', 'skills'], ['increase', 'graduation_rate'], ['pay', 'personnel'], ['textbooks', 'supplies'], ['parent_involvement'], ['lea', 'parent_involvement', 'policy'], ['school', 'parent_compact'], ['parents', 'right', 'know', 'requirement'], ['parent', 'inputsupport'], ['click', 'here', 'school', 'parent', 'student', 'compact'], ['dont_want', 'to', 'add', 'corrections', 'perfect', 'website', 'writes', 'papers'], ['all', 'students', 'professor', 'needs'], ['website', 'writes', 'papers'], [], ['some', 'final', 'tips', 'if', 'want', 'sooner', 'allows', 'succeed', 'academic', 'assignmentsour', 'skilled', 'writer', 'customer_satisfaction', 'samples', 'friends', 'groupmates', 'relatives', 'time'], ['richfield', 'public', 'school', 'academy'], ['district'], ['about', 'us'], ['district', 'improvement'], ['richfield_elc'], ['relc', 'improvement_plan'], ['relc', 'drill', 'reporting'], ['richfield_psa'], ['rpsa', 'improvement_plan'], ['rpsa', 'drill', 'reporting'], ['enroll', 'now'], ['contact_us'], ['join', 'perfect_attendance', 'group', 'support', 'child', 'qualify', 'raffle', 'great', 'prizes'], [], ['richfield', 'public', 'school', 'academy'], ['richfield', 'early', 'learning', 'center'], ['elga_credit', 'union', 'partnership'], ['enroll', 'now'], ['extra_curricular', 'activities'], ['attendance', 'policy'], ['richfield', 'public', 'school', 'academy', 'approved', 'changes', 'attendance', 'policy'], ['click', 'links', 'view', 'documents', 'related', 'changes'], ['attendance', 'letter'], ['attendance', 'letter', 'spanish'], ['attendance', 'policy'], ['attendance', 'policy', 'spanish'], ['dress_code', 'cell', 'phone', 'policy', 'changes'], ['richfield', 'public', 'school', 'academy', 'approved', 'changes', 'dress_code', 'cell', 'policies'], ['click', 'links', 'view', 'documents', 'related', 'changes'], ['dress_code', 'policy', 'changes'], ['cell', 'phone', 'policy', 'changes'], ['rpsa', 'middle', 'school', 'career', 'fair'], ['monday'], ['mission', 'the', 'mission', 'richfield', 'public', 'school', 'academy', 'create', 'high', 'standard', 'academic_excellence', 'students', 'grow', 'become_lifelong', 'learners'], ['richfield', 'academy', 'preschool'], ['w', 'atherton_road'], ['flint'], ['office'], ['fax'], ['school', 'hours'], ['richfield', 'early', 'learning', 'center', 'grades_k'], ['richfield', 'road'], ['flint'], ['office'], ['fax'], ['school', 'hours'], ['richfield', 'public', 'school', 'academy', 'grades'], ['north', 'center', 'road'], ['flint'], ['office'], ['fax'], ['school', 'hours'], ['contact_us'], ['public', 'notice', 'board', 'member', 'vacancy'], ['richfield', 'public', 'school', 'academy', 'seeking_qualified', 'candidates', 'vacancies', 'board_directors'], ['if', 'interested_applying', 'nomination', 'one', 'vacancies', 'please', 'contact', 'school'], ['thank'], ['all', 'rpsa', 'board', 'applications', 'reviewed', 'current', 'board_directors', 'approved', 'bay_mills', 'community', 'college']]\n"
     ]
    }
   ],
   "source": [
    "# ## Preprocessing I: Tokenize web text by sentences\n",
    "\n",
    "df = quickpickle_load(charters_path) # Load charter data into DF\n",
    "print(\"DF loaded from \" + str(charters_path) + \"...\")\n",
    "\n",
    "if phrased: \n",
    "    pass # If parsed sentence phrase data exists, don't bother with tokenizing sentences\n",
    "\n",
    "elif sented: # Check if tokenized sentence data already exists. If so, don't bother reparsing it\n",
    "    words_by_sentence = []\n",
    "    \n",
    "    # Load data back in for parsing phrases and word embeddings model:\n",
    "    if mpdo:\n",
    "        words_by_sentence = load_tokslist(wordsent_path) \n",
    "    else:\n",
    "        words_by_sentence = quickpickle_load(wordsent_path) \n",
    "\n",
    "else:\n",
    "    \n",
    "    words_by_sentence = [] # Initialize variable to hold list of lists of words (sentences)\n",
    "    pcount=0 # Initialize preprocessing counter\n",
    "    df[\"WEBTEXT\"] = df[\"WEBTEXT\"].astype(list) # Coerce these to lists in order to avoid type errors\n",
    "\n",
    "    # Convert DF into list (of lists of tuples) and call preprocess_wem on element each using Pool():\n",
    "    try:\n",
    "        tqdm.pandas(desc=\"Tokenizing sentences\") # To show progress, create & register new `tqdm` instance with `pandas`\n",
    "\n",
    "        # WITH multiprocessing (faster):\n",
    "        if mpdo:\n",
    "            weblist = df[\"WEBTEXT\"].tolist() # Convert DF into list to pass to Pool()\n",
    "\n",
    "            # Use multiprocessing.Pool(numcpus) to run preprocess_wem:\n",
    "            print(\"Preprocessing web text into list of sentences...\")\n",
    "            if __name__ == '__main__':\n",
    "                with Pool(numcpus) as p:\n",
    "                    p.map(preprocess_wem, tqdm(weblist, desc=\"Tokenizing sentences\")) \n",
    "\n",
    "        # WITHOUT multiprocessing (much slower):\n",
    "        else:\n",
    "            df[\"WEBTEXT\"].progress_apply(lambda tups: preprocess_wem(tups))\n",
    "\n",
    "            # Save data for later\n",
    "            try: # Use quickpickle to dump data into pickle file\n",
    "                if __name__ == '__main__': \n",
    "                    print(\"Saving list of tokenized sentences to file...\")\n",
    "                    t = timeit.Timer(stmt=\"quickpickle_dump(words_by_sentence, wordsent_path)\", globals=globals())\n",
    "                    print(\"Time elapsed saving data: \" + str(round(t.timeit(1),4)),'\\n')\n",
    "\n",
    "                '''with open(wordsent_path, 'wb') as destfile:\n",
    "                    gc.disable() # Disable garbage collector to increase speed\n",
    "                    cPickle.dump(words_by_sentence, destfile)\n",
    "                    gc.enable() # Enable garbage collector again'''\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e), \"\\nTrying backup save option...\")\n",
    "                try:\n",
    "                    # Slower way to save data:\n",
    "                    with open(wordsent_path, 'wb') as destfile:\n",
    "                        t = timeit.Timer(stmt=\"pickle.dump(words_by_sentence, destfile)\", globals=globals())\n",
    "                        print(\"Success! Time elapsed saving data: \" + str(round(t.timeit(1),4)),'\\n')\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to save sentence data: \" + str(e))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to tokenize sentences: \" + str(e))\n",
    "        sys.exit()\n",
    "        \n",
    "    \n",
    "# ## Preprocessing II: Detect and parse common phrases in words_by_sentence\n",
    "\n",
    "if phrased: # Check if phrased data already exists. If so, don't bother recalculating it\n",
    "    words_by_sentence = []\n",
    "    words_by_sentence = quickpickle_load(phrasesent_path) # Load data back in, for word embeddings model\n",
    "\n",
    "else:\n",
    "    tqdm.pandas(desc=\"Parsing phrases\") # Change title of tqdm instance\n",
    "\n",
    "    try:\n",
    "        print(\"Detecting and parsing phrases in list of sentences...\")\n",
    "        # Threshold represents a threshold for forming the phrases (higher means fewer phrases). A phrase of words a and b is accepted if (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold, where N is the total vocabulary size. By default this value is 10.0\n",
    "        phrases = Phrases(words_by_sentence, min_count=3, delimiter=b'_', common_terms=stop_word_list, threshold=8) # Detect phrases in sentences based on collocation counts\n",
    "        words_by_sentence = [phrases[sent] for sent in tqdm(words_by_sentence, desc=\"Parsing phrases\")] # Apply phrase detection model to each sentence in data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse sentence phrases: \" + str(e))\n",
    "        sys.exit()\n",
    "    \n",
    "    # Save data for later\n",
    "    try: # Use quickpickle to dump data into pickle file\n",
    "        if __name__ == '__main__': \n",
    "            print(\"Saving list of tokenized, phrased sentences to file...\")\n",
    "            t = timeit.Timer(stmt=\"quickpickle_dump(words_by_sentence, phrasesent_path)\", globals=globals())\n",
    "            print(\"Time elapsed saving data: \" + str(round(t.timeit(1),4)),'\\n')\n",
    "                                         \n",
    "        '''with open(phrasesent_path, 'wb') as destfile:\n",
    "            gc.disable() # Disable garbage collector to increase speed\n",
    "            cPickle.dump(words_by_sentence, destfile)\n",
    "            gc.enable() # Enable garbage collector again'''\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e), \"\\nTrying backup save option...\")\n",
    "        try:\n",
    "            # Slower way to save data:\n",
    "            with open(phrasesent_path, 'wb') as destfile:\n",
    "                t = timeit.Timer(stmt=\"pickle.dump(words_by_sentence, destfile)\", globals=globals())\n",
    "                print(\"Success! Time elapsed saving data: \" + str(round(t.timeit(1),4)),'\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Failed to save parsed sentence phrases: \" + str(e))\n",
    "        \n",
    "\n",
    "# Take a look at the data \n",
    "print(\"Sample of the first 150 sentences:\")\n",
    "print(words_by_sentence[:150])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext = df['WEBTEXT']\n",
    "webtext_with_text = []\n",
    "for t in webtext:\n",
    "    try:\n",
    "        len_site = len(t)\n",
    "        #initializing a master string\n",
    "        school_string = ''\n",
    "        for i in range(len_site):\n",
    "            str_edit = t[i][3]\n",
    "            str_edit.replace('\\r', ' ')\n",
    "            str_edit.replace(r\"[^\\w\\s]\", ' ')\n",
    "            str_edit.replace(r\"\\s+\", ' ')\n",
    "            school_string += str_edit + \" \"\n",
    "        webtext_with_text.append(school_string)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tagged = []\n",
    "for doc in webtext_with_text:\n",
    "    doc_split = doc.split()\n",
    "    T = gensim.models.doc2vec.TaggedDocument(doc_split,[doc])\n",
    "    docs_tagged.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tagged[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up multiprocessing\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Train the model with above parameters:\n",
    "print(\"Training doc2vec model...\")\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab(docs_tagged)\n",
    "print(\"doc2vec model TRAINED successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.most_similar('discipline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model \n",
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model_dbow.save(fname)\n",
    "#for loading\n",
    "#model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_sentslist = [] # Initialize variable to hold list of lists of words (sentences)\n",
    "def preprocess_wem(tuplist): # inputs were formerly: (tuplist, start, limit)\n",
    "    \n",
    "    '''This function cleans and tokenizes sentences, removing punctuation and numbers and making words into lower-case stems.\n",
    "    Inputs: list of four-element tuples, the last element of which holds the long string of text we care about;\n",
    "        an integer limit (bypassed when set to -1) indicating the DF row index on which to stop the function (for testing purposes),\n",
    "        and similarly, an integer start (bypassed when set to -1) indicating the DF row index on which to start the function (for testing purposes).\n",
    "    This function loops over five nested levels, which from high to low are: row, tuple, chunk, sentence, word.\n",
    "    Note: This approach maintains accurate semantic distances by keeping stopwords.'''\n",
    "        \n",
    "    global mpdo # Check if we're doing multiprocessing. If so, then mpdo=True\n",
    "    global sents_combined # Grants access to variable holding a list of lists of words, where each list of words represents a sentence in its original order (only relevant for this function if we're not using multiprocessing)\n",
    "    global pcount # Grants access to preprocessing counter\n",
    "    \n",
    "    known_pages = set() # Initialize list of known pages for a school\n",
    "    sents_combined = [] # Initialize list of all school's sentences\n",
    "\n",
    "    if type(tuplist)==float:\n",
    "        return # Can't iterate over floats, so exit\n",
    "    \n",
    "    #print('Parsing school #' + str(pcount)) # Print number of school being parsed\n",
    "\n",
    "    for tup in tuplist: # Iterate over tuples in tuplist (list of tuples)\n",
    "        if tup[3] in known_pages or tup=='': # Could use hashing to speed up comparison: hashlib.sha224(tup[3].encode()).hexdigest()\n",
    "            continue # Skip this page if exactly the same as a previous page on this school's website\n",
    "\n",
    "        for chunk in tup[3].split('\\n'): \n",
    "            for sent in sent_tokenize(chunk): # Tokenize chunk by sentences (in case >1 sentence in chunk)\n",
    "                sent = clean_sentence(sent) # Clean and tokenize sentence\n",
    "                \n",
    "                if ((sent == []) or (len(sent) == 0)): # If sentence is empty, continue to next sentence without appending\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # TO DO: Chunk this by school, not just sentence\n",
    "                # TO DO: Now that sentences are parsed and cleaned by spaces, \n",
    "                # recombine and then parse more accurately using spacy word tokenizer\n",
    "                \n",
    "                # Save preprocessing sentence to object (if not multiprocessing)\n",
    "                sents_combined.append(sent) # add sent to object\n",
    "                    \n",
    "        known_pages.add(tup[3])\n",
    "        \n",
    "    school_sentslist.append(sents_combined) # add sent to object\n",
    "    \n",
    "    #pcount += 1 # Add to counter\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_wem(df['WEBTEXT'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_list = unicode_make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['imag'],\n",
       "  ['imag'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['staff', 'members'],\n",
       "  ['img'],\n",
       "  ['img'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['th', 'th'],\n",
       "  ['mrs', 'melissa', 'johnson'],\n",
       "  ['mrs', 'melissa', 'johnson'],\n",
       "  ['melissa', 'johnson', 'th-th', 'grade'],\n",
       "  ['my', 'name', 'melissa', 'johnson', 'i', 'born', 'raised', 'ketchikan'],\n",
       "  ['i', 'graduated', 'ketchikan', 'high', 'school', ''],\n",
       "  ['i', 'enjoy', 'hiking', 'fishing'],\n",
       "  ['after',\n",
       "   'high',\n",
       "   'school',\n",
       "   'i',\n",
       "   'went',\n",
       "   'college',\n",
       "   'i',\n",
       "   'played',\n",
       "   'basketball',\n",
       "   'volleyball'],\n",
       "  ['my', 'husband', 'kevin', 'family', 'moved', 'back', 'ketchikan', ''],\n",
       "  ['i',\n",
       "   'three',\n",
       "   'amazing',\n",
       "   'children',\n",
       "   'isaac',\n",
       "   'senior',\n",
       "   'kayhi',\n",
       "   'krey',\n",
       "   'junior',\n",
       "   'bree',\n",
       "   'th',\n",
       "   'grader',\n",
       "   'tsas'],\n",
       "  ['i', 'school', 'administrator', 'teacher', 'coach', 'years'],\n",
       "  ['i',\n",
       "   'bachelors',\n",
       "   'degree',\n",
       "   'science',\n",
       "   'education',\n",
       "   'masters',\n",
       "   'educational',\n",
       "   'leadership'],\n",
       "  ['areas', 'education', 'i', 'specialize', 'reading', 'writing'],\n",
       "  ['welcome', 'th', 'th'],\n",
       "  ['th', 'th', 'home'],\n",
       "  ['th', 'th'],\n",
       "  ['', 'theme-relationship'],\n",
       "  ['th', 'th', 'homework', 'policy'],\n",
       "  ['th', 'th', 'newsnotes'],\n",
       "  ['thth', 'student', 'class', 'supply', 'list'],\n",
       "  ['independent', 'reading', 'project'],\n",
       "  ['mathematics'],\n",
       "  ['mrs', 'madonna', 'hall'],\n",
       "  ['mrs', 'melissa', 'johnson'],\n",
       "  ['power'],\n",
       "  ['th', 'th', 'news'],\n",
       "  ['th', 'th', 'links'],\n",
       "  ['anthropological', 'association'],\n",
       "  ['historical', 'society'],\n",
       "  ['science', 'center'],\n",
       "  ['american', 'museum', 'natural', 'history'],\n",
       "  ['online', 'classroom'],\n",
       "  ['smithsonian', 'institution'],\n",
       "  ['wikipedia'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['staff', 'members'],\n",
       "  ['adam'],\n",
       "  ['adam'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['report', 'card'],\n",
       "  ['report', 'card'],\n",
       "  ['assessment',\n",
       "   'tongass',\n",
       "   'school',\n",
       "   'students',\n",
       "   'take',\n",
       "   'standardized',\n",
       "   'tests',\n",
       "   'required',\n",
       "   'state',\n",
       "   'department',\n",
       "   'education',\n",
       "   'early',\n",
       "   'development'],\n",
       "  ['these',\n",
       "   'currently',\n",
       "   'include',\n",
       "   'state',\n",
       "   'benchmark',\n",
       "   'tests',\n",
       "   'terra',\n",
       "   'nova'],\n",
       "  ['the',\n",
       "   'school',\n",
       "   'integrates',\n",
       "   'student',\n",
       "   'assessment',\n",
       "   'within',\n",
       "   'teaching',\n",
       "   'process',\n",
       "   'order',\n",
       "   'plan',\n",
       "   'next',\n",
       "   'steps',\n",
       "   'instruction'],\n",
       "  ['assessment',\n",
       "   'embedded',\n",
       "   'instructional',\n",
       "   'process',\n",
       "   'order',\n",
       "   'avoid',\n",
       "   'interruptions',\n",
       "   'student',\n",
       "   'learning'],\n",
       "  ['the',\n",
       "   'school',\n",
       "   'provides',\n",
       "   'variety',\n",
       "   'ways',\n",
       "   'students',\n",
       "   'show',\n",
       "   'learned',\n",
       "   'learned',\n",
       "   'using',\n",
       "   'methods',\n",
       "   'known',\n",
       "   'authentic',\n",
       "   'performance',\n",
       "   'assessment'],\n",
       "  ['the',\n",
       "   'emphasis',\n",
       "   'continual',\n",
       "   'growth',\n",
       "   'progress',\n",
       "   'individual',\n",
       "   'students'],\n",
       "  ['children',\n",
       "   'along',\n",
       "   'parents',\n",
       "   'teachers',\n",
       "   'important',\n",
       "   'partners',\n",
       "   'evaluating',\n",
       "   'student',\n",
       "   'achievement'],\n",
       "  ['the',\n",
       "   'assessment',\n",
       "   'methods',\n",
       "   'tongass',\n",
       "   'school',\n",
       "   'based',\n",
       "   'following',\n",
       "   'four',\n",
       "   'assessment',\n",
       "   'principles'],\n",
       "  ['in',\n",
       "   'order',\n",
       "   'complete',\n",
       "   'picture',\n",
       "   'students',\n",
       "   'growth',\n",
       "   'different',\n",
       "   'types',\n",
       "   'assessments',\n",
       "   'must',\n",
       "   'used'],\n",
       "  ['assessments',\n",
       "   'focus',\n",
       "   'students',\n",
       "   'growth',\n",
       "   'towards',\n",
       "   'high',\n",
       "   'standard',\n",
       "   'rather',\n",
       "   'comparing',\n",
       "   'students',\n",
       "   'performance',\n",
       "   'students'],\n",
       "  ['there',\n",
       "   'close',\n",
       "   'relationship',\n",
       "   'desired',\n",
       "   'student',\n",
       "   'outcome',\n",
       "   'means',\n",
       "   'used',\n",
       "   'assess'],\n",
       "  ['assessing',\n",
       "   'students',\n",
       "   'knowledge',\n",
       "   'important',\n",
       "   'assessing',\n",
       "   'knowledge'],\n",
       "  ['assessment',\n",
       "   'promote',\n",
       "   'support',\n",
       "   'reflection',\n",
       "   'self-evaluation',\n",
       "   'part',\n",
       "   'students',\n",
       "   'staff',\n",
       "   'parents'],\n",
       "  ['the', 'tongass', 'school', 'evaluates', 'records', 'student', 'progress'],\n",
       "  ['continuums',\n",
       "   'academic',\n",
       "   'growth',\n",
       "   'reading',\n",
       "   'writing',\n",
       "   'mathematics',\n",
       "   'sciences'],\n",
       "  ['personal', 'creative', 'interests', 'talents', 'inventories'],\n",
       "  ['portfolios',\n",
       "   'progress',\n",
       "   'major',\n",
       "   'academic',\n",
       "   'arts',\n",
       "   'areas',\n",
       "   'including',\n",
       "   'student',\n",
       "   'self-assessment',\n",
       "   'growth'],\n",
       "  ['demonstrationsperformances', 'achievement'],\n",
       "  ['teacher',\n",
       "   'anecdotal',\n",
       "   'records',\n",
       "   'evaluative',\n",
       "   'rubrics',\n",
       "   'narrative',\n",
       "   'reports'],\n",
       "  ['parent', 'observations', 'surveys'],\n",
       "  ['student-led', 'conferences', 'parents', 'teachers'],\n",
       "  ['welcome'],\n",
       "  ['school', 'office'],\n",
       "  ['school'],\n",
       "  ['enrollment'],\n",
       "  ['our', 'program'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'vision'],\n",
       "  ['photo', 'gallery'],\n",
       "  ['report', 'card'],\n",
       "  ['school', 'forms'],\n",
       "  ['school', 'newsletter'],\n",
       "  ['staff', 'members'],\n",
       "  ['announcements'],\n",
       "  ['external', 'school', 'links'],\n",
       "  ['ketchikan', 'school', 'district'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['imag'],\n",
       "  ['imag'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['preschool-kindergarten', 'multiage', 'class'],\n",
       "  ['pictures', 'throughout', 'watch', 'us', 'grow'],\n",
       "  ['pictures', 'throughout', 'watch', 'us', 'grow'],\n",
       "  ['first', 'day', 'of', 'school'],\n",
       "  [''],\n",
       "  ['star', 'student'],\n",
       "  [''],\n",
       "  ['welcome', 'kindergarten'],\n",
       "  ['kindergarten', 'home'],\n",
       "  ['kindergarten'],\n",
       "  ['being', 'there', 'experiences'],\n",
       "  ['kindergarten', 'news'],\n",
       "  ['mrs', 'harmony', 'rushton'],\n",
       "  ['ms', 'alexis', 'mccolley-edwardson'],\n",
       "  ['pictures', 'throughout', 'watch', 'us', 'grow'],\n",
       "  ['kindergarten', 'news'],\n",
       "  ['kindergarten', 'links'],\n",
       "  [''],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['imag'],\n",
       "  ['imag'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['apc', 'home'],\n",
       "  ['meeting', 'minutes', 'agendas'],\n",
       "  ['', 'apc', 'meeting', 'minutes'],\n",
       "  ['', 'apc', 'meeting', 'minutes'],\n",
       "  ['septemberapcdraftminutes'],\n",
       "  ['welcome', 'apc'],\n",
       "  ['apc', 'home'],\n",
       "  ['apc'],\n",
       "  ['annual', 'report'],\n",
       "  ['committee', 'members'],\n",
       "  ['meeting', 'minutes', 'agendas'],\n",
       "  ['policy', 'manual'],\n",
       "  ['', 'apc', 'meeting', 'minutes'],\n",
       "  ['apc', 'news'],\n",
       "  ['apc', 'links'],\n",
       "  [''],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['current', 'happenings'],\n",
       "  ['before', 'after', 'school', 'program', 'schedule'],\n",
       "  ['before', 'after', 'school', 'program', 'schedule'],\n",
       "  ['before', 'school', 'program', 'start', 'time'],\n",
       "  ['after',\n",
       "   'school',\n",
       "   'program',\n",
       "   '',\n",
       "   'we',\n",
       "   'homework',\n",
       "   'club',\n",
       "   'gym',\n",
       "   'much',\n",
       "   'much'],\n",
       "  ['welcome'],\n",
       "  ['school', 'office'],\n",
       "  ['school'],\n",
       "  ['enrollment'],\n",
       "  ['our', 'program'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'vision'],\n",
       "  ['photo', 'gallery'],\n",
       "  ['report', 'card'],\n",
       "  ['school', 'forms'],\n",
       "  ['school', 'newsletter'],\n",
       "  ['staff', 'members'],\n",
       "  ['announcements'],\n",
       "  ['external', 'school', 'links'],\n",
       "  ['ketchikan', 'school', 'district'],\n",
       "  ['the', 'sidebar', 'added', 'widgets'],\n",
       "  ['please', 'add'],\n",
       "  ['widgets', 'page'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['st', 'grade'],\n",
       "  ['mrs', 'carol', 'stanton'],\n",
       "  ['mrs', 'carol', 'stanton'],\n",
       "  ['miss', 'carol'],\n",
       "  ['welcome', 'st'],\n",
       "  ['st', 'home'],\n",
       "  ['st'],\n",
       "  ['st', 'homework', 'policy'],\n",
       "  ['st', 'news', 'note'],\n",
       "  ['st', 'photo', 'gallery'],\n",
       "  ['stnd', 'student', 'class', 'supply', 'list'],\n",
       "  ['mrs', 'allyson', 'sebcioglu'],\n",
       "  ['mrs', 'carol', 'stanton'],\n",
       "  ['st', 'news'],\n",
       "  ['st', 'links'],\n",
       "  ['google', 'maps'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['preschool-kindergarten', 'multiage', 'class'],\n",
       "  ['mrs', 'harmony', 'rushton'],\n",
       "  ['mrs', 'harmony', 'rushton'],\n",
       "  ['mrs', 'rushton', 'welcomes', 'preschool', 'kindergarten', 'families'],\n",
       "  ['welcome', 'kindergarten'],\n",
       "  ['kindergarten', 'home'],\n",
       "  ['kindergarten'],\n",
       "  ['being', 'there', 'experiences'],\n",
       "  ['kindergarten', 'news'],\n",
       "  ['mrs', 'harmony', 'rushton'],\n",
       "  ['ms', 'alexis', 'mccolley-edwardson'],\n",
       "  ['pictures', 'throughout', 'watch', 'us', 'grow'],\n",
       "  ['kindergarten', 'news'],\n",
       "  ['kindergarten', 'links'],\n",
       "  [''],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['music'],\n",
       "  ['ms', 'kelly', 'burke'],\n",
       "  ['ms', 'kelly', 'burke'],\n",
       "  ['welcome'],\n",
       "  ['my', 'name', 'kelly', 'burke', 'i', 'excited', 'new', 'music', 'teacher'],\n",
       "  ['i', 'born', 'raised', 'chicago'],\n",
       "  ['i',\n",
       "   'began',\n",
       "   'musical',\n",
       "   'adventure',\n",
       "   'singing',\n",
       "   'schools',\n",
       "   'childrens',\n",
       "   'choir',\n",
       "   'joining',\n",
       "   'band',\n",
       "   'playing',\n",
       "   'clarinet'],\n",
       "  ['i',\n",
       "   'received',\n",
       "   'bachelors',\n",
       "   'degree',\n",
       "   'music',\n",
       "   'education',\n",
       "   'vandercook',\n",
       "   'college',\n",
       "   'music'],\n",
       "  ['there',\n",
       "   'i',\n",
       "   'learned',\n",
       "   'play',\n",
       "   'every',\n",
       "   'instrument',\n",
       "   'conduct',\n",
       "   'teach',\n",
       "   'every',\n",
       "   'kind',\n",
       "   'ensemble',\n",
       "   'music',\n",
       "   'class'],\n",
       "  ['i',\n",
       "   'also',\n",
       "   'developed',\n",
       "   'proficiencies',\n",
       "   'instruments',\n",
       "   'like',\n",
       "   'voice',\n",
       "   'viola',\n",
       "   'french',\n",
       "   'horn'],\n",
       "  ['what', 'else', 'know'],\n",
       "  ['well', 'i', 'love', 'theater'],\n",
       "  ['i',\n",
       "   'plays',\n",
       "   'musicals',\n",
       "   'throughout',\n",
       "   'time',\n",
       "   'grade',\n",
       "   'school',\n",
       "   'high',\n",
       "   'school',\n",
       "   'college'],\n",
       "  ['i',\n",
       "   'also',\n",
       "   'currently',\n",
       "   'playing',\n",
       "   'clarinet',\n",
       "   'ketchikan',\n",
       "   'community',\n",
       "   'concert',\n",
       "   'band'],\n",
       "  ['being', 'able', 'teach', 'children', 'exciting', 'opportunity'],\n",
       "  ['i', 'thrilled', 'able', 'watch', 'help', 'grow', 'young', 'musicians'],\n",
       "  ['musically'],\n",
       "  ['kelly', 'burke'],\n",
       "  ['welcome', 'music'],\n",
       "  ['music', 'home'],\n",
       "  ['music'],\n",
       "  ['st', '', 'grade', 'music'],\n",
       "  ['ms', 'kelly', 'burke'],\n",
       "  ['pre-school', '', 'kindergarten'],\n",
       "  ['spring', 'program'],\n",
       "  ['what', 'orff'],\n",
       "  ['music', 'news'],\n",
       "  ['music', 'links'],\n",
       "  [''],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['stop',\n",
       "   'tongass',\n",
       "   'school',\n",
       "   'office',\n",
       "   'anytime',\n",
       "   'let',\n",
       "   'us',\n",
       "   'personally',\n",
       "   'welcome',\n",
       "   'tongass',\n",
       "   'school'],\n",
       "  ['we',\n",
       "   'happy',\n",
       "   'give',\n",
       "   'tour',\n",
       "   'school',\n",
       "   'would',\n",
       "   'pleased',\n",
       "   'tell',\n",
       "   'award-winning',\n",
       "   'program'],\n",
       "  ['dr', 'marian', 'gonzales', 'principal'],\n",
       "  ['mrsrobin', 'velazquez', 'administrative', 'assistant'],\n",
       "  ['cindy', 'moody', 'health', 'aide'],\n",
       "  ['welcome'],\n",
       "  ['school', 'office'],\n",
       "  ['school'],\n",
       "  ['enrollment'],\n",
       "  ['our', 'program'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'vision'],\n",
       "  ['photo', 'gallery'],\n",
       "  ['report', 'card'],\n",
       "  ['school', 'forms'],\n",
       "  ['school', 'newsletter'],\n",
       "  ['staff', 'members'],\n",
       "  ['announcements'],\n",
       "  ['external', 'school', 'links'],\n",
       "  ['ketchikan', 'school', 'district'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['school', 'announcements'],\n",
       "  ['kindergarten',\n",
       "   'enrollment',\n",
       "   'open',\n",
       "   '',\n",
       "   'school',\n",
       "   'year',\n",
       "   'call',\n",
       "   '',\n",
       "   'packet',\n",
       "   'info'],\n",
       "  ['updated', ''],\n",
       "  ['posted'],\n",
       "  ['current', 'happenings'],\n",
       "  ['before', 'after', 'school', 'program', 'schedule'],\n",
       "  ['updated', ''],\n",
       "  ['posted'],\n",
       "  ['current', 'happenings'],\n",
       "  ['before',\n",
       "   'school',\n",
       "   'program',\n",
       "   'start',\n",
       "   'time',\n",
       "   'after',\n",
       "   'school',\n",
       "   'program',\n",
       "   '',\n",
       "   'we',\n",
       "   'homework',\n",
       "   'club',\n",
       "   'gym',\n",
       "   'much',\n",
       "   'much'],\n",
       "  ['tongass', 'school', 'designated', 'school', 'excellence'],\n",
       "  ['updated', ''],\n",
       "  ['posted'],\n",
       "  ['school', 'announcements'],\n",
       "  ['love'],\n",
       "  ['tweet'],\n",
       "  ['tongass',\n",
       "   'school',\n",
       "   'one',\n",
       "   'schools',\n",
       "   'entire',\n",
       "   'nation',\n",
       "   'selected',\n",
       "   'school',\n",
       "   'excellence',\n",
       "   ''],\n",
       "  ['m'],\n",
       "  ['t'],\n",
       "  ['w'],\n",
       "  ['t'],\n",
       "  ['f'],\n",
       "  ['s'],\n",
       "  ['s'],\n",
       "  ['dec'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['imag'],\n",
       "  ['imag'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['back', 'tongass', 'school', 'arts', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['our', 'school', 'office'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['tongass', 'school', 'arts', 'sciences'],\n",
       "  ['title', 'i', 'program', 'parent', 'information'],\n",
       "  ['dear', 'families'],\n",
       "  ['tongass', 'school', 'school-wide', 'title', 'i', 'school'],\n",
       "  ['this',\n",
       "   'means',\n",
       "   'tongass',\n",
       "   'school',\n",
       "   'receives',\n",
       "   'federal',\n",
       "   'grant',\n",
       "   'money',\n",
       "   'assist',\n",
       "   'us',\n",
       "   'ensuring',\n",
       "   'children',\n",
       "   'meet',\n",
       "   'challenging',\n",
       "   'state',\n",
       "   'academic',\n",
       "   'content',\n",
       "   'student',\n",
       "   'academic',\n",
       "   'achievement',\n",
       "   'standards'],\n",
       "  ['title',\n",
       "   'i',\n",
       "   'federal',\n",
       "   'grant',\n",
       "   'program',\n",
       "   'designed',\n",
       "   'give',\n",
       "   'financial',\n",
       "   'assistance',\n",
       "   'state',\n",
       "   'local',\n",
       "   'education',\n",
       "   'agencies',\n",
       "   'qualifying',\n",
       "   'public',\n",
       "   'schools'],\n",
       "  ['the',\n",
       "   'ketchikan',\n",
       "   'gateway',\n",
       "   'borough',\n",
       "   'school',\n",
       "   'district',\n",
       "   'provides',\n",
       "   'tongass',\n",
       "   'school',\n",
       "   'title',\n",
       "   'i',\n",
       "   'funds'],\n",
       "  ['in',\n",
       "   'past',\n",
       "   'funds',\n",
       "   'used',\n",
       "   'hire',\n",
       "   'additional',\n",
       "   'staff',\n",
       "   'purchase',\n",
       "   'instructional',\n",
       "   'materials',\n",
       "   'provide',\n",
       "   'professional',\n",
       "   'development',\n",
       "   'support',\n",
       "   'students',\n",
       "   'reading',\n",
       "   'writing',\n",
       "   'math',\n",
       "   'able',\n",
       "   'grow',\n",
       "   'proficiency',\n",
       "   'reading',\n",
       "   'math',\n",
       "   'state',\n",
       "   'assessments'],\n",
       "  ['title',\n",
       "   'i',\n",
       "   'schools',\n",
       "   'required',\n",
       "   'develop',\n",
       "   'parent',\n",
       "   'engagement',\n",
       "   'policy',\n",
       "   'specifically',\n",
       "   'outlines',\n",
       "   'school',\n",
       "   'support',\n",
       "   'parents',\n",
       "   'students',\n",
       "   'learning'],\n",
       "  ['click'],\n",
       "  ['here'],\n",
       "  ['open', 'download', 'copy', 'policy'],\n",
       "  ['a',\n",
       "   'title',\n",
       "   'i',\n",
       "   'school-wide',\n",
       "   'program',\n",
       "   'plan',\n",
       "   'developed',\n",
       "   'tongass',\n",
       "   'school'],\n",
       "  ['copies', 'current', 'plan', 'available', 'office'],\n",
       "  ['teachers',\n",
       "   'parents',\n",
       "   'community',\n",
       "   'members',\n",
       "   'meet',\n",
       "   'year',\n",
       "   'revise',\n",
       "   'plan'],\n",
       "  ['families', 'view', 'school-wide', 'plan', 'following', 'website'],\n",
       "  ['click', 'login', 'button', 'page'],\n",
       "  ['next', 'enter', 'guests', 'username', 'guests', 'password'],\n",
       "  ['please',\n",
       "   'watch',\n",
       "   'information',\n",
       "   'upcoming',\n",
       "   'news',\n",
       "   'notes',\n",
       "   'notification',\n",
       "   'dates',\n",
       "   'times',\n",
       "   'review',\n",
       "   'meetings',\n",
       "   'held'],\n",
       "  ['thank', 'you'],\n",
       "  ['dr', 'marian', 'gonzales'],\n",
       "  ['principal'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['homework', 'policy'],\n",
       "  ['homework', 'policy'],\n",
       "  ['expectations', 'daily', 'homework'],\n",
       "  ['students',\n",
       "   'given',\n",
       "   'homework',\n",
       "   'math',\n",
       "   'language',\n",
       "   'arts',\n",
       "   'monday',\n",
       "   'thursday'],\n",
       "  ['students', 'encouraged', 'submit', 'homework', 'morning'],\n",
       "  ['students',\n",
       "   'get',\n",
       "   'homework',\n",
       "   'turned',\n",
       "   'regular',\n",
       "   'basis',\n",
       "   'given',\n",
       "   'opportunities',\n",
       "   'complete',\n",
       "   'homework',\n",
       "   'open',\n",
       "   'time'],\n",
       "  ['students',\n",
       "   'also',\n",
       "   'make',\n",
       "   'weekly',\n",
       "   'homework',\n",
       "   'home',\n",
       "   'submit',\n",
       "   'class',\n",
       "   'partial',\n",
       "   'credit'],\n",
       "  ['we',\n",
       "   'encourage',\n",
       "   'students',\n",
       "   'complete',\n",
       "   'homework',\n",
       "   'daily',\n",
       "   'basis',\n",
       "   'anything',\n",
       "   'assist',\n",
       "   'student',\n",
       "   'please',\n",
       "   'let',\n",
       "   'us',\n",
       "   'know'],\n",
       "  ['rd', 'grade', 'homework', 'expectations', 'minutes', 'mon-thurs'],\n",
       "  ['th', 'grade', 'homework', 'expectations', 'minutes', 'mon-thurs'],\n",
       "  ['please',\n",
       "   'let',\n",
       "   'know',\n",
       "   'questions',\n",
       "   'students',\n",
       "   'homework',\n",
       "   'expectations',\n",
       "   'thank',\n",
       "   'home'],\n",
       "  ['mr', 'shultz', 'mrs', 'rauwolf'],\n",
       "  ['welcome', 'rd', 'th'],\n",
       "  ['rd', 'th', 'home'],\n",
       "  ['rd', 'th'],\n",
       "  ['th', 'grade', 'math', 'mrs', 'rauwolf'],\n",
       "  ['being', 'there', 'experiences'],\n",
       "  ['celebration', 'learning'],\n",
       "  ['class', 'supply', 'list'],\n",
       "  ['classroom', 'pictures'],\n",
       "  ['current', 'newsnote'],\n",
       "  ['elizabeth', 'petratrovich', 'celebration'],\n",
       "  ['homework', 'policy'],\n",
       "  ['mr', 'clint', 'shultz'],\n",
       "  ['mrs'],\n",
       "  ['dawn', 'rauwolf'],\n",
       "  ['social', 'action', 'project'],\n",
       "  ['stop', 'motion', 'animation'],\n",
       "  ['tongass', 'treasures'],\n",
       "  ['welcome', 'letter'],\n",
       "  ['rd', 'th', 'news'],\n",
       "  ['rd', 'th', 'links'],\n",
       "  ['community', 'profiles'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['daily', 'school', 'schedule'],\n",
       "  ['daily', 'school', 'schedule'],\n",
       "  ['', 'k', 'before-school', 'program', 'starts'],\n",
       "  ['', 'k', 'student', 'day', 'starts'],\n",
       "  ['', 'peer', 'buddy', 'preschool', 'arrives'],\n",
       "  ['', 'prek', 'student', 'day', 'starts'],\n",
       "  ['', 'recess'],\n",
       "  [''],\n",
       "  ['', 'peer', 'buddy', 'preschool', 'student', 'day', 'ends'],\n",
       "  ['', 'prek', 'k-th', 'grade', 'student', 'day', 'ends'],\n",
       "  ['', 'k', 'after-school', 'program', 'ends'],\n",
       "  ['search'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ['head', 'sprout', 'mimio'],\n",
       "  ['odyssey'],\n",
       "  ['district', 'links'],\n",
       "  ['kgbsd', 'district'],\n",
       "  ['tongass', 'school', 'buses'],\n",
       "  ['online', 'payment'],\n",
       "  ['powerschool', 'parent', 'portal'],\n",
       "  ['contact', 'us', 'today'],\n",
       "  ['schoenbar', 'road'],\n",
       "  ['ketchikan'],\n",
       "  ['phone', '', ''],\n",
       "  ['fax', '', ''],\n",
       "  ['email'],\n",
       "  ['search'],\n",
       "  ['write', 'us', 'email'],\n",
       "  ['copyright', '', 'tongass', 'school', 'arts', '', 'sciences'],\n",
       "  ['l', 'enrollment', 'homeschool'],\n",
       "  ['title', 'ix', 'assuring', 'gender', 'equity', 'education'],\n",
       "  ['title', 'i', 'parent', 'involvement', 'information'],\n",
       "  ['unpaid', 'meal', 'policy'],\n",
       "  ['our', 'program'],\n",
       "  ['report', 'card'],\n",
       "  [''],\n",
       "  ['our', 'vision'],\n",
       "  ['school', 'philosophy'],\n",
       "  ['homepage'],\n",
       "  ['classrooms'],\n",
       "  ['rd', 'th', 'grade'],\n",
       "  ['imag'],\n",
       "  ['imag'],\n",
       "  ['about', 'us'],\n",
       "  ['our', 'schools', 'philosophy'],\n",
       "  ['our', 'schools', 'mission'],\n",
       "  ['report', 'card'],\n",
       "  ['student', 'enrollment'],\n",
       "  ['at', 'home', 'support'],\n",
       "  ['khan', 'academy', 'math'],\n",
       "  ...]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "school_sentslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
