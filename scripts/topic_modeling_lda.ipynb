{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text\n",
    "import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../nowdata/parsing/overlaps_removed_df.csv', sep = \"\\t\", low_memory=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WEBTEXT'] = data['WEBTEXT'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'icator Dashboard\\nOur goal is to create 75,000 credentials/degrees by 2030. This can only happen when the entire community is purposeful through data in establishing and enabling conditions for long-term attainment success.\\nCCI Dashboard Sponsored in part by the J.L. Bedsole Foundation\\nHome\\n | \\nAbout\\n | \\nOur Work\\n | \\nOur Impact\\n | \\nNews/ Media\\n | \\nSupport\\n | \\nContact\\nÂ© 2018 Copyright Mobile Area Education Foundation.  All rights reserved. | Site by \\n Web and New Media\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0,\"WEBTEXT\"][2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WEBTEXT'] = data['WEBTEXT'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webtext = data[\"WEBTEXT\"]\n",
    "#webtext_with_text = []\n",
    "#for t in webtext:\n",
    "#    try:\n",
    "#        len_site = len(t)\n",
    "#        for i in range(len_site):\n",
    "#            webtext_with_text.append(t[i][3])\n",
    "#    except:\n",
    "#        pass\n",
    "\n",
    "webtext = data[\"WEBTEXT\"]\n",
    "webtext_with_text = []\n",
    "tokens = []\n",
    "for t in webtext:\n",
    "    try:\n",
    "        len_site = len(t)\n",
    "        #initializing a master string\n",
    "        school_string = ''\n",
    "        for i in range(len_site):\n",
    "            school_string += t[i][3] + ' \\n '\n",
    "        webtext_with_text.append(school_string)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "# creating custom stop words\n",
    "my_additional_stop_word_list = []\n",
    "#long list of manual addition to stopwords:\n",
    "for i in range(1,13):\n",
    "    my_additional_stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "for i in range(1,13):\n",
    "    my_additional_stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "for i in range(1, 2100):\n",
    "    my_additional_stop_word_list.append(str(i))\n",
    "my_additional_stop_word_list.append('00')\n",
    "my_additional_stop_word_list.append('el')\n",
    "my_additional_stop_word_list.append('en')\n",
    "my_additional_stop_word_list.append('la')\n",
    "my_additional_stop_word_list.append('los')\n",
    "my_additional_stop_word_list.append('para')\n",
    "my_additional_stop_word_list.append('las')\n",
    "my_additional_stop_word_list.append('san')\n",
    "my_additional_stop_word_list.append('mr')\n",
    "my_additional_stop_word_list.append('mrs')\n",
    "my_additional_stop_word_list.append('sa')\n",
    "my_additional_stop_word_list.append('angeles')\n",
    "my_additional_stop_word_list.append('diego')\n",
    "my_additional_stop_word_list.append('california')\n",
    "my_additional_stop_word_list.append('york')\n",
    "my_additional_stop_word_list.append('fax')\n",
    "my_additional_stop_word_list.append('email')\n",
    "my_additional_stop_word_list.append('phone')\n",
    "my_additional_stop_word_list.append('harlem')\n",
    "my_additional_stop_word_list.append('bronx')\n",
    "my_additional_stop_word_list.append('wi')\n",
    "my_additional_stop_word_list.append('pm')\n",
    "my_additional_stop_word_list.append('org')\n",
    "my_additional_stop_word_list.append('city')\n",
    "my_additional_stop_word_list.append('austin')\n",
    "my_additional_stop_word_list.append('antonio')\n",
    "my_additional_stop_word_list.append('texas')\n",
    "#adding states abbreviations for stopwords\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\"]\n",
    "for state in states:\n",
    "          my_additional_stop_word_list.append(state)\n",
    "for state in [state.lower() for state in states]:\n",
    "          my_additional_stop_word_list.append(state)\n",
    "stop_words_extra = text.ENGLISH_STOP_WORDS.union(my_additional_stop_word_list)\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=0.02, max_features=no_features, stop_words=stop_words_extra)\n",
    "tf = tf_vectorizer.fit_transform(webtext_with_text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# No. of topics - 20\n",
    "no_topics = 20\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='batch', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: idea schools college public regional act amended office suite title academy children teacher prep south campus services free principal national\n",
      "Topic 1: board meeting click montessori schools value agenda child minutes children enrollment community newsletter office parent program day year read calendar\n",
      "Topic 2: board charter pdf minutes meeting committee year report academy parents file program state parent plan agendas information schools financial shall\n",
      "Topic 3: academies dallas story read reserved home rights copyright careers design organization support big non news alumni programs events web life\n",
      "Topic 4: high life skills public charter notice info information news enrollment schools diploma career new board locations request apply program learn\n",
      "Topic 5: schools learning charter online homeschool help stem dyslexia method summer curriculum service bullying problems class study funding data project home\n",
      "Topic 6: academy success schools scholars read year scholar high new parents middle teacher approach charter prospective careers grade apply day stay\n",
      "Topic 7: course college high university arts year courses program science grade information class work skills time curriculum online academy math state\n",
      "Topic 8: schools charter news valley new high public summit college team comments teachers year support south community work content join read\n",
      "Topic 9: learning online charter home center parent com support study schools community learn https curriculum program classes free enroll time board\n",
      "Topic 10: charter send teacher high academy powered schools edlio middle homework science calendar ave parent directions copy message choosing portal st\n",
      "Topic 11: teacher grade university years teaching year high program science team community new college middle time degree grades family english board\n",
      "Topic 12: calendar events facebook campus academy news high parent search lunch twitter schedule information directory board home google resources form athletics\n",
      "Topic 13: district website harmony elementary community programs rights schools policy discrimination information title origin national disability color race activities west ix\n",
      "Topic 14: charter academy view information day board schools events children community questions join privacy national website legal river street site comments\n",
      "Topic 15: district elementary information high services k12 online board home creek schools policy state community privacy center new county learning enroll\n",
      "Topic 16: information district child parent parents grade services teacher year program policy use children board office support teachers schools service provide\n",
      "Topic 17: prep college community program arizona arts high campus year donate leadership university new summer programs posted news schools calendar family\n",
      "Topic 18: site close map blackboard web community terms feedback copyright privacy policy questions use center reserved manager updated rights youtube learning\n",
      "Topic 19: grade day week calendar math class year information kindergarten 8th teacher parent science com reading read time 5th 6th events\n"
     ]
    }
   ],
   "source": [
    "#old version\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic \" + str(topic_idx) + ': ' + \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "no_top_words = 20\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "#topics to note: topic 1, topic 2, topic 4, topic , topic 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-02bd018c9a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "# creating custom stop words\n",
    "my_additional_stop_word_list = []\n",
    "#long list of manual addition to stopwords:\n",
    "for i in range(1,13):\n",
    "    my_additional_stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "for i in range(1,13):\n",
    "    my_additional_stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "for i in range(1, 2100):\n",
    "    my_additional_stop_word_list.append(str(i))\n",
    "my_additional_stop_word_list.append('00')\n",
    "my_additional_stop_word_list.append('el')\n",
    "my_additional_stop_word_list.append('en')\n",
    "my_additional_stop_word_list.append('la')\n",
    "my_additional_stop_word_list.append('los')\n",
    "my_additional_stop_word_list.append('para')\n",
    "my_additional_stop_word_list.append('las')\n",
    "my_additional_stop_word_list.append('san')\n",
    "my_additional_stop_word_list.append('mr')\n",
    "my_additional_stop_word_list.append('mrs')\n",
    "my_additional_stop_word_list.append('sa')\n",
    "my_additional_stop_word_list.append('angeles')\n",
    "my_additional_stop_word_list.append('diego')\n",
    "my_additional_stop_word_list.append('california')\n",
    "my_additional_stop_word_list.append('york')\n",
    "my_additional_stop_word_list.append('fax')\n",
    "my_additional_stop_word_list.append('email')\n",
    "my_additional_stop_word_list.append('phone')\n",
    "my_additional_stop_word_list.append('harlem')\n",
    "my_additional_stop_word_list.append('bronx')\n",
    "my_additional_stop_word_list.append('wi')\n",
    "my_additional_stop_word_list.append('pm')\n",
    "my_additional_stop_word_list.append('org')\n",
    "my_additional_stop_word_list.append('city')\n",
    "my_additional_stop_word_list.append('austin')\n",
    "my_additional_stop_word_list.append('antonio')\n",
    "my_additional_stop_word_list.append('texas')\n",
    "#adding states abbreviations for stopwords\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\"]\n",
    "for state in states:\n",
    "          my_additional_stop_word_list.append(state)\n",
    "for state in [state.lower() for state in states]:\n",
    "          my_additional_stop_word_list.append(state)\n",
    "stop_words_extra = text.ENGLISH_STOP_WORDS.union(my_additional_stop_word_list)\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=0.02, max_features=no_features, stop_words=stop_words_extra)\n",
    "tf = tf_vectorizer.fit_transform(webtext_with_text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of topics - 10\n",
    "no_topics = 10\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='batch', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old version\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic \" + str(topic_idx) + ': ' + \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "no_top_words = 20\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
