{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import math\n",
    "#from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from difflib import SequenceMatcher as SeqMatcher\n",
    "import numpy as np\n",
    "\n",
    "folder_prefix = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_pickle(folder_prefix+ 'charters_full_2015.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['values', 'academics', 'skills', 'purpose',\n",
    "                       'direction', 'mission', 'vision', 'vision', 'mission', 'our purpose',\n",
    "                       'our ideals', 'ideals', 'our cause', 'curriculum','curricular',\n",
    "                       'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system',\n",
    "                       'structure','philosophy', 'philosophical', 'beliefs', 'believe',\n",
    "                       'principles', 'creed', 'credo', 'values','moral', 'history', 'our story',\n",
    "                       'the story', 'school story', 'background', 'founding', 'founded',\n",
    "                       'established','establishment', 'our school began', 'we began',\n",
    "                       'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "                       'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = ['mission','vision', 'vision:', 'mission:', 'our purpose', 'our ideals', 'ideals:', 'our cause', 'cause:', 'goals', 'objective']\n",
    "curriculum_keywords = ['curriculum', 'curricular', 'program', 'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system', 'structure']\n",
    "philosophy_keywords = ['philosophy', 'philosophical', 'beliefs', 'believe', 'principles', 'creed', 'credo', 'value',  'moral']\n",
    "history_keywords = ['history', 'story','our story', 'the story', 'school story', 'background', 'founding', 'founded', 'established', 'establishment', 'our school began', 'we began', 'doors opened', 'school opened']\n",
    "about_keywords =  ['about us', 'our school', 'who we are', 'overview', 'general information', 'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = set(stemmer.stem(word) for word in mission_keywords)\n",
    "curriculum_keywords = set(stemmer.stem(word) for word in curriculum_keywords)\n",
    "philosophy_keywords = set(stemmer.stem(word) for word in philosophy_keywords)\n",
    "history_keywords = set(stemmer.stem(word) for word in history_keywords)\n",
    "about_keywords =  set(stemmer.stem(word) for word in about_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc = new_data.columns.get_loc('WEBTEXT')\n",
    "# new_data.iloc[:, [loc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #all_pages is a list of strings of all web page texts\n",
    "# all_pages = []\n",
    "\n",
    "# no_nan_data = new_data.[0:7]dropna(subset=['WEBTEXT'])\n",
    "# col_pages = no_nan_data['WEBTEXT'] #change to new_data['data'] later, but work with first 1 schools for now\n",
    "# i = 0\n",
    "# count = 0\n",
    "# for school_data in col_pages:\n",
    "\n",
    "#     for tup in school_data:\n",
    "#         all_pages.append(tup[3])\n",
    "      \n",
    "\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data['WEBTEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sim_df(pages):\n",
    "    \n",
    "    ratio_df = pd.DataFrame(np.array(pages), columns=[\"Page\"])\n",
    "    ratio_df[\"Ratios\"] = np.nan\n",
    "    ratio_df[\"Similar\"] = np.nan #similar will hold similar indexes, > 0.90 ratios\n",
    "\n",
    "    ratio_df['Ratios'] = ratio_df['Ratios'].astype('object')\n",
    "    ratio_df['Similar'] = ratio_df['Similar'].astype('object')\n",
    "    \n",
    "    index = 0\n",
    "    for page0 in pages:\n",
    "        ratios_list = []\n",
    "        for page1 in pages:\n",
    "            ratios_list.append(SeqMatcher(None, page0, page1).ratio())\n",
    "        ratio_df['Ratios'][index] = ratios_list\n",
    "        sim_ind_list = np.asarray(np.where((np.asarray(ratios_list) >= 0.5) & (np.asarray(ratios_list) != 1.0))[0]).tolist()\n",
    "        ratio_df['Similar'][index] = sim_ind_list\n",
    "        index+=1\n",
    "                \n",
    "    num_rows = ratio_df.shape[0] \n",
    "    tot_tuples = []\n",
    "\n",
    "    final_cut_strings = [None] * num_rows\n",
    "    count_not_similar = 0  \n",
    "    not_sim_list = []\n",
    "    sim_list = []\n",
    "\n",
    "    for r in range(num_rows):\n",
    "        if (len(ratio_df['Similar'][r]) != 0):\n",
    "            sim_list.append(r)\n",
    "            #print(\"has similar > 0.9 at index : \" + str(r))\n",
    "            list_cut = []\n",
    "            for ind in ratio_df['Similar'][r]:\n",
    "                list_of_triples = SeqMatcher(None, ratio_df['Page'][r], ratio_df['Page'][ind]).get_matching_blocks()\n",
    "                zeroth_triple = list_of_triples[0] #first triple, most likely in beginning, most likely a header\n",
    "                n = zeroth_triple[2] #j to j+n are the indices of the overlapping part\n",
    "                orig_string = ratio_df['Page'][ind]\n",
    "                cut_down_string = orig_string[:zeroth_triple[1]] + orig_string[zeroth_triple[1] + n:] #removes overlapping part\n",
    "                list_cut.append([ind, cut_down_string])\n",
    "            tot_tuples.extend(list_cut) #list of tuples\n",
    "        else:\n",
    "            #print(\"no similar at index : \" + str(r))\n",
    "            not_sim_list.append(r)\n",
    "            count_not_similar +=1\n",
    "            final_cut_strings[r]= ratio_df['Page'][r]\n",
    "    \n",
    "    return tot_tuples, final_cut_strings, ratio_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# for page0 in unique_pages_set:\n",
    "#     ratios_list = []\n",
    "#     for page1 in unique_pages_set:\n",
    "#         ratios_list.append(SeqMatcher(None, page0, page1).ratio())\n",
    "#     ratio_df['Ratios'][index] = ratios_list\n",
    "#     sim_ind_list = np.asarray(np.where((np.asarray(ratios_list) > 0.9) & (np.asarray(ratios_list) != 1.0))[0]).tolist()\n",
    "#     ratio_df['Similar'][index] = sim_ind_list\n",
    "#     index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows = ratio_df.shape[0] \n",
    "# tot_tuples = []\n",
    "\n",
    "# final_cut_strings = [None] * num_rows\n",
    "# count_not_similar = 0  \n",
    "# not_sim_list = []\n",
    "# sim_list = []\n",
    "\n",
    "# for r in range(num_rows):\n",
    "#     if (len(ratio_df['Similar'][r]) != 0):\n",
    "#         sim_list.append(r)\n",
    "#         #print(\"has similar > 0.9 at index : \" + str(r))\n",
    "#         list_cut = []\n",
    "#         for ind in ratio_df['Similar'][r]:\n",
    "#             list_of_triples = SeqMatcher(None, ratio_df['Page'][r], ratio_df['Page'][ind]).get_matching_blocks()\n",
    "#             zeroth_triple = list_of_triples[0] #first triple, most likely in beginning, most likely a header\n",
    "#             n = zeroth_triple[2] #j to j+n are the indices of the overlapping part\n",
    "#             orig_string = ratio_df['Page'][ind]\n",
    "#             cut_down_string = orig_string[:zeroth_triple[1]] + orig_string[zeroth_triple[1] + n:] #removes overlapping part\n",
    "#             list_cut.append([ind, cut_down_string])\n",
    "#         tot_tuples.extend(list_cut) #list of tuples\n",
    "#     else:\n",
    "#         #print(\"no similar at index : \" + str(r))\n",
    "#         not_sim_list.append(r)\n",
    "#         count_not_similar +=1\n",
    "#         final_cut_strings[r]= ratio_df['Page'][r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_header_cut(tot_tuples, final_cut_strings, num_rows):\n",
    "    #num_rows = ratio_df.shape[0]\n",
    "    list_grouped = [[] for x in range(num_rows)] #big list, just put list of strings in spots where needed\n",
    "    list_indices_of_groups = []\n",
    "    for ind in range(num_rows):\n",
    "        for tup in tot_tuples:\n",
    "            if(tup[0] == ind):\n",
    "                list_grouped[ind].append(tup[1]) #attach that tuple's string\n",
    "                list_indices_of_groups.append(tup[0])\n",
    "                \n",
    "    ind_fill_final_grouped = []\n",
    "    i = 0\n",
    "    for group in list_grouped:\n",
    "        if (len(group) != 0):\n",
    "            ind_fill_final_grouped.append(i)\n",
    "    \n",
    "        i+=1\n",
    "        \n",
    "    spot = 0\n",
    "    #add into final cut strings, the new \"cut down\" versions of appropriate strings\n",
    "    for li in list_grouped:\n",
    "        if (len(li) != 0):\n",
    "            #print(unique_ind_list[spot])\n",
    "            final_cut_strings[ind_fill_final_grouped[spot]]= min(li, key=len) #inserts into correct index, what was None before, add in that string now\n",
    "            spot+=1\n",
    "    \n",
    "    return final_cut_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_grouped = [[] for x in range(num_rows)] #big list, just put list of strings in spots where needed\n",
    "# list_indices_of_groups = []\n",
    "# for ind in range(num_rows):\n",
    "#     for tup in tot_tuples:\n",
    "#         if(tup[0] == ind):\n",
    "#             list_grouped[ind].append(tup[1]) #attach that tuple's string\n",
    "#             list_indices_of_groups.append(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used\n",
    "#unique_group_ind = set(list_indices_of_groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_fill_final_grouped = []\n",
    "# i = 0\n",
    "# for group in list_grouped:\n",
    "#     if (len(group) != 0):\n",
    "#         #print(\"list ready to insert at index \" + str(i) + \"\\n\")\n",
    "#         ind_fill_final_grouped.append(i)\n",
    "    \n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot = 0\n",
    "# #add into final cut strings, the new \"cut down\" versions of appropriate strings\n",
    "# for li in list_grouped:\n",
    "#     if (len(li) != 0):\n",
    "#         #print(unique_ind_list[spot])\n",
    "#         final_cut_strings[ind_fill_final_grouped[spot]]= min(li, key=len) #inserts into correct index, what was None before, add in that string now\n",
    "#         spot+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_header_cut(first_header_cut):\n",
    "    \n",
    "    super_final_strings = []\n",
    "    for s in first_header_cut:\n",
    "        s_new = \"\"\n",
    "        if(s is None):\n",
    "            s_new = \"\"\n",
    "        else:\n",
    "            s_new = s\n",
    "        use_punc = False\n",
    "        use_sev = False\n",
    "        punc = [\",\", \".\", \":\", \";\"]\n",
    "        p_list = []\n",
    "        for p in punc:\n",
    "            if (s_new.find(p) != -1):\n",
    "                p_list.append(s_new.find(p))\n",
    "            else:\n",
    "                p_list.append(len(s_new))\n",
    "\n",
    "        punc_ind = min(p_list)\n",
    "\n",
    "        n_list = [index for index, k in enumerate(s_new) if k=='\\n']\n",
    "        start_punc = len(s_new)\n",
    "        for i in n_list:\n",
    "            if(i < punc_ind):\n",
    "                start_punc = i # start_punc equals the largest index of \\n that's less than index of first punctuation\n",
    "\n",
    "        start = 0\n",
    "        end = 0\n",
    "        total = \"\"\n",
    "        list_totals = []\n",
    "        st_en = []\n",
    "        for c in s_new:\n",
    "            if (c not in ['\\n', '\\t']):\n",
    "                total+=(c)\n",
    "                end+=1\n",
    "            else :\n",
    "                if(len(total.split()) >= 7): # we hit 7 words or more, wipe everything before start index\n",
    "                    #list_totals.append(total)\n",
    "                    st_en.append((start, end))\n",
    "\n",
    "                total= \"\"\n",
    "                start = end\n",
    "        start_sev = len(s_new)-1 #len(s)-1 #make it huge by default; if there's no group of 7, then start punc will be the smallest\n",
    "        if len(st_en) > 0:\n",
    "            start_sev = st_en[0][0] #index of first group of words that's >= 7 words; 0th tuple's start value\n",
    "\n",
    "\n",
    "        #take smaller of the two indices, since we want to use the property which occurs first\n",
    "        if start_punc < start_sev:\n",
    "            #if start of sentence which ends in/contains puncuation occurs earlier, wipe eveything before that index\n",
    "            #only take that index +1 and on, start right after the new line\n",
    "            new_string = s_new[start_punc+1:] \n",
    "            super_final_strings.append(new_string)\n",
    "\n",
    "        else:\n",
    "            #if start of group of words that >= 7 occurs earlier than a sentence with punctuation, wipe eveything before that index\n",
    "            #only take that index and on, statr using that begining of the group of 7+ words\n",
    "            new_string = s_new[start_sev:] \n",
    "            super_final_strings.append(new_string)\n",
    "        \n",
    "    return super_final_strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first removal of headers in final_cut_Strings currently, but now we want to cut down headers more\n",
    "#take out text before the first sentence or text before the first group of 7+ words\n",
    "\n",
    "# super_final_strings = []\n",
    "# for s in final_cut_strings:\n",
    "#     use_punc = False\n",
    "#     use_sev = False\n",
    "#     punc = [\",\", \".\", \":\", \";\"]\n",
    "#     p_list = []\n",
    "#     for p in punc:\n",
    "#         if (s.find(p) != -1):\n",
    "#             p_list.append(s.find(p))\n",
    "#         else:\n",
    "#             p_list.append(len(s))\n",
    "    \n",
    "#     punc_ind = min(p_list)\n",
    "    \n",
    "#     n_list = [index for index, k in enumerate(s) if k=='\\n']\n",
    "#     start_punc = len(s)\n",
    "#     for i in n_list:\n",
    "#         if(i < punc_ind):\n",
    "#             start_punc = i # start_punc equals the largest index of \\n that's less than index of first punctuation\n",
    "    \n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     total = \"\"\n",
    "#     list_totals = []\n",
    "#     st_en = []\n",
    "#     for c in s:\n",
    "#         if (c not in ['\\n', '\\t']):\n",
    "#             total+=(c)\n",
    "#             end+=1\n",
    "#         else :\n",
    "#             if(len(total.split()) >= 7): # we hit 7 words or more, wipe everything before start index\n",
    "#                 #list_totals.append(total)\n",
    "#                 st_en.append((start, end))\n",
    "    \n",
    "#             total= \"\"\n",
    "#             start = end\n",
    "#     start_sev = len(s)-1 #len(s)-1 #make it huge by default; if there's no group of 7, then start punc will be the smallest\n",
    "#     if len(st_en) > 0:\n",
    "#         start_sev = st_en[0][0] #index of first group of words that's >= 7 words; 0th tuple's start value\n",
    "   \n",
    "\n",
    "#     #take smaller of the two indices, since we want to use the property which occurs first\n",
    "#     if start_punc < start_sev:\n",
    "#         #if start of sentence which ends in/contains puncuation occurs earlier, wipe eveything before that index\n",
    "#         #only take that index +1 and on, start right after the new line\n",
    "#         new_string = s[start_punc+1:] \n",
    "#         super_final_strings.append(new_string)\n",
    "        \n",
    "#     else:\n",
    "#         #if start of group of words that >= 7 occurs earlier than a sentence with punctuation, wipe eveything before that index\n",
    "#         #only take that index and on, statr using that begining of the group of 7+ words\n",
    "#         new_string = s[start_sev:] \n",
    "#         super_final_strings.append(new_string)\n",
    "        \n",
    "     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list of common words in footers\n",
    "# footer_list = [\"Copyright\", \"All Rights Reserved\",  \"Read More\", \n",
    "#                \"Useful Links\", \"Search\", \"Survey\", \"Feed\", \"Phone\", \"Fax\", \"Address\",  \"Sitemap\", \n",
    "#                \"Jobs\", \"Apply\", \"Pre-Enroll\"]\n",
    "# #facebook, contact us, enroll etc occurs in headers as well, so had to take that out\n",
    "\n",
    "# footers_removed_strings = []\n",
    "\n",
    "# #look for the keyword in each string and if found, remove all the text after it\n",
    "# for s in super_final_strings:\n",
    "#     no_newline = s.replace(\"\\n\", \" \")\n",
    "#     new_list = []\n",
    "#     for word in footer_list:\n",
    "#         if (no_newline.find(word) != -1):\n",
    "#             new_list.append(no_newline.find(word))\n",
    "#         else :\n",
    "#             new_list.append(len(s))\n",
    "\n",
    "#     #get the index of the earliest occurence of a keyword\n",
    "#     f_ind = min(new_list)\n",
    "    \n",
    "#     #go back to new line or period right before and wipe out everything after that\n",
    "#     n_list = [index for index, k in enumerate(s) if k in ['\\n', '.']]\n",
    "#     start_punc = len(s)\n",
    "#     for i in n_list:\n",
    "#         if(i < f_ind):\n",
    "#             start_punc = i # start_punc equals the largest index of \\n or . that's less than index of the keyword\n",
    "            \n",
    "#     if start_punc < f_ind:\n",
    "#             footer_rem = s[:start_punc]\n",
    "#             footers_removed_strings.append(footer_rem) \n",
    "#     else:\n",
    "#             footer_rem = s[:f_ind]\n",
    "#             footers_removed_strings.append(footer_rem) \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#footers_removed_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#footers_removed_strings are the final, most cut down versions of the web text\n",
    "#we did process on \"WEBTEXT\" column of new_data\n",
    "#now repeat for \"CMO_WEBTEXT\" column of new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_strings = pd.DataFrame(footers_removed_strings, columns=[\"Cleaned Text\"])\n",
    "# cleaned_strings.to_csv('CleanedStrings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare between pages of each school\n",
    "\n",
    "def remove_string_overlaps(tuplist):\n",
    "   \n",
    "    unique_tuplist = []\n",
    "    seen_pages = set() # Initialize list of known pages for a school\n",
    "    unique_pages=[]\n",
    "    reversed_pages = []\n",
    "    tup_indices = []\n",
    "\n",
    "    cleaned_strings = []\n",
    "    for tup in tuplist:\n",
    "        seen_pages.add(tup[3])\n",
    "\n",
    "    for i in range(len(tuplist)):\n",
    "        if(tuplist[i][3] in seen_pages and (tuplist[i][3]  not in unique_pages) and (tuplist[i][3] is not None)):\n",
    "            unique_tuplist.append(tuplist[i])\n",
    "            unique_pages.append(tuplist[i][3])\n",
    "            reversed_pages.append(tuplist[i][3][::-1])\n",
    "            tup_indices.append(i)\n",
    "            #print(\"unique page : \" + str(i))\n",
    "\n",
    "    #now compare all pages with each other \n",
    "    #print(unique_tuplist)\n",
    "    tot_tuples, final_cut_strings, ratio_df = create_sim_df(unique_pages)\n",
    "    first_header_cut = create_first_header_cut(tot_tuples, final_cut_strings, ratio_df.shape[0])\n",
    "    \n",
    "    #first removal of headers in final_cut_Strings currently, but now we want to cut down headers more\n",
    "    #take out text before the first sentence or text before the first group of 7+ words  \n",
    "    second_header_cut = create_second_header_cut(first_header_cut)\n",
    "    \n",
    "    \n",
    "    #now run process on reversed strings\n",
    "    \n",
    "    rev_tot_tuples, rev_final_cut_strings, rev_ratio_df = create_sim_df(reversed_pages)\n",
    "    \n",
    "    rev_first_header_cut = create_first_header_cut(rev_tot_tuples, rev_final_cut_strings, rev_ratio_df.shape[0])\n",
    "    \n",
    "    for i in range(len(rev_first_header_cut)):\n",
    "        #find where to cut the footer off , index\n",
    "        \n",
    "        add_string = second_header_cut[i]\n",
    "        if i in tup_indices: \n",
    "            forward_string = rev_first_header_cut[i][::-1]\n",
    "            #print(forward_string)\n",
    "            sept = int(len(forward_string)/2)\n",
    "            half_string = forward_string[len(forward_string) - sept:]\n",
    "            #find that half in the regular string, and get the end of the half\n",
    "            #print(type(second_header_cut[i]))\n",
    "            #print(type(half_string))\n",
    "            end_index = (second_header_cut[i]).find(half_string) + len(half_string) - 1\n",
    "            add_string = second_header_cut[i][:end_index]\n",
    "            #remove the footer aka remove stuff after the end_index\n",
    "            #keep the stuff before end_index\n",
    "        \n",
    "        cleaned_strings.append(add_string)\n",
    "\n",
    "\n",
    "    #         for tup in tuplist: # Iterate over tuples in tuplist (list of tuples)\n",
    "    #             if tup[3] in known_pages or tup=='': # Compare this page with previous ones\n",
    "    #             continue # Or better yet, remove tuple\n",
    "    #             # CLEAN STRING HERE\n",
    "    #             known_pages.add(tup[3]) # Add this page to pages we've already seen\n",
    "    \n",
    "    #then iterate through cleaned_strings and inset into each tuple\n",
    "    new_list = []\n",
    "    for count in range(len(cleaned_strings)):\n",
    "        new_tup = (tuplist[tup_indices[count]][0], tuplist[tup_indices[count]][1], tuplist[tup_indices[count]][2], cleaned_strings[count])\n",
    "        new_list.append(new_tup)\n",
    "#         print(tup_indices[count])\n",
    "#         print(cleaned_strings[count])\n",
    "#         print(\"\\n  \\n\")\n",
    "\n",
    "#     print(len(new_list))\n",
    "#     print(len(tup_indices))\n",
    "    return new_list\n",
    "\n",
    "\n",
    "#first make a list of tuples that has no nan values and has\n",
    "#if not np.isnan(tuplist):   CHECK FOR NAN OUTSIDE\n",
    "\n",
    "#k = remove_string_overlaps(new_data['WEBTEXT'][11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply remove_string_overlaps on each school, aka on each row of new_data\n",
    "#since pages of a school will likely be similar to the other pages within that school own "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_df(old_list):\n",
    "    #print(loc)\n",
    "    new_list = remove_string_overlaps(old_list)\n",
    "    return new_list\n",
    "    \n",
    "# loc = new_data.columns.get_loc('WEBTEXT')\n",
    "# # new_data.iloc[:, [loc]]\n",
    "# temp = new_data.iloc[11:13]\n",
    "# temp['WEBTEXT'] = temp['WEBTEXT'].apply(parse_df)\n",
    "# temp['WEBTEXT'][0]\n",
    "\n",
    "new_data['WEBTEXT'] = new_data['WEBTEXT'].apply(parse_df)\n",
    "new_data.to_pickle('new_removed_overlaps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = [1,2,3,4]\n",
    "\n",
    "# def multiply(li):\n",
    "#     return [3*x for x in li]\n",
    "\n",
    "# def plus_2(old_list):\n",
    "#     new_list = multiply(old_list) \n",
    "#     old_list = new_list\n",
    "#    # print(old_list)\n",
    "#     #print(new_list)\n",
    "    \n",
    "# k= plus_2(k)\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data['WEBTEXT'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"Skip to Main Content\\nToggle main menu visibility \\nHome\\nAbout Us\\nAbout Our School\\nOur History\\nAdministration\\nBoard Members\\nSchools\\nHouston Elementary\\nHouston Middle School\\nLancaster Campus\\nRegistration\\nCalendar\\nNews\\nDepartments\\nPreschool\\nFood Services\\nHuman Resources\\nHelpful Links\\nContact Us\\nACCELERATED INTERDISCIPLINARY INTERMEDIATE ACADEMY\\n“The school where every challenge becomes a mission accomplished.\\n”\\nContact Us\\nWe at AIA welcome your questions and comments. Please feel free to contact us via e-mail or using the methods listed below.\\nOur Address\\nAIA District\\nP.O. Box 20589\\nHouston, Texas 77225-0589\\nAIA District E-mail\\nOur Houston Locations\\nElementary School\\n12825 Summit Ridge\\nHouston, TX 77085\\nPhone: (713) 728-9330\\nMiddle School\\n12825 Summit Ridge\\nHouston, TX 77085\\nPhone: (713) 283-6298\\nOur Lancaster Location\\nElementary School\\n901 East Beltline Rd.\\nLancaster, TX 75146\\nPhone: (972) 227-2105\\n*Note: E-mail resumes and enrollment information for AIA Lancaster Elem to the \\nHuman Resources Department\\n.\\nOur Office Hours\\n7:15 a.m. - 5:00 p.m. Monday through Friday\\nParent Resources\\nParent and Student Handbook\\nEnrollment Forms\\nRegistration\\nSchool Menus\\nOur Schools\\n\\r\\t\\t\\t\\t\\t\\tCopyright ©2018\\xa0| Site designed and maintained by \\nSchool Webmasters\"\n",
    "# half_string = s[int(len(s)/2):]\n",
    "# end_index = s.find(half_string) + len(half_string) - 1\n",
    "# end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data['WEBTEXT'][9][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_string_overlaps(new_data['WEBTEXT'][9])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplist = new_data['WEBTEXT'][0]\n",
    "# seen_pages = set()\n",
    "# k = []\n",
    "# for tup in tuplist:\n",
    "#     seen_pages.add(tup[3])\n",
    "\n",
    "# index = 0\n",
    "# for tup in tuplist:\n",
    "#     if tup[3] in seen_pages and tup[3] not in k:\n",
    "#         k.append(tup[3])\n",
    "#         print(index)\n",
    "#     else:\n",
    "#         print(\"the repeated indices : \" + str(index))\n",
    "#     index+=1\n",
    "# #len(seen_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplist[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplist[1][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
